https://keda.sh/docs/2.17/reference/

| ç»„ä»¶åç§° | Pod åç§° | åŠŸèƒ½ | å¤‡æ³¨ | 
| -- | -- | -- | -- |
| keda-metrics-apiserver | keda-operator-metrics-apiserver | æä¾›  | å½“å‰æ ‡å‡†éƒ¨ç½²æ–¹å¼ | 
| keda-operator | keda-operator | ç®¡ç† ScaledObjectã€è§¦å‘æ‰©ç¼©å®¹ | æ§åˆ¶å™¨ç»„ä»¶ | 
| keda-admission-webhooks | keda-admission-webhooks | éªŒè¯å’Œä¿®æ”¹ CRD | å¯é€‰ä½†æ¨è | 


```
Â graph TD
Â  Â  subgraph Kubernetes Cluster
Â  Â  Â  Â  A[metrics-server] -->|æä¾› CPU/å†…å­˜æŒ‡æ ‡| B[HPA]
Â  Â  Â  Â  B -->|æ§åˆ¶å‰¯æœ¬æ•°| D[Deployment/Pod]

Â  Â  Â  Â  C[Prometheus] -->|æŠ“å–è‡ªå®šä¹‰æŒ‡æ ‡| E[keda-operator-metrics-apiserver]
Â  Â  Â  Â  E -->|æä¾› external metrics API| B

Â  Â  Â  Â  H[ScaledObject] -->|å®šä¹‰ Triggerï¼ˆåŒ…æ‹¬ resourceï¼‰| F[keda-operator]
Â  Â  Â  Â  F -->|è§¦å‘æ‰©ç¼©å®¹| D

Â  Â  Â  Â  E -.->|external.metrics.k8s.io æŸ¥è¯¢æŒ‡æ ‡å<br/>ï¼ˆç”± ScaledObject å®šä¹‰ï¼‰| H
Â  Â  Â  Â  A -.->|é€šè¿‡ resource trigger é—´æ¥ä½¿ç”¨<br/>ï¼ˆç”± KEDA è½¬æ¢ï¼‰| H

Â  Â  Â  Â  I[keda-admission-webhooks] -->|éªŒè¯ & æ³¨å…¥é»˜è®¤å€¼| H
Â  Â  end

Â  Â  subgraph å¤–éƒ¨ç³»ç»Ÿ
Â  Â  Â  Â  G[Kafka / Redis / HTTP / etc.] -->|äº‹ä»¶/æŒ‡æ ‡| H
Â  Â  end 
```

![](images/WEBRESOURCE3aeb432e1053dc16f7a72963e904db22image.png)

ScaledObject

ä½¿ç”¨metric serveræ—¶ï¼Œ è¿™é‡Œé¢å®ƒåªä¼šå»å–scaleTargetRefæŒ‡å®šçš„cpu,è€Œä¸æ˜¯é›†ç¾¤æ‰€æœ‰çš„cpu

ä¸‹é¢æ˜¯ä½¿ç”¨prometheus

```shell
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: ets-qe-scaledobject
  labels:
    app: ets-qe
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: ets-qe
  pollingInterval: 30                                      # Optional, the interval KEDA to check each trigger on. Default: 30 seconds
  advanced: # Optional. Section to specify advanced options
    restoreToOriginalReplicaCount: true          # Optional. Default: false
    horizontalPodAutoscalerConfig: # Optional. Section to specify HPA related options
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: Percent
              value: 20
              periodSeconds: 300
            - type: Pods
              value: 1
              periodSeconds: 300
        scaleUp:
          stabilizationWindowSeconds: 60
          policies:
            - type: Percent
              value: 20
              periodSeconds: 600
            - type: Pods
              value: 1
              periodSeconds: 300
          selectPolicy: Max
  minReplicaCount: 2
  maxReplicaCount: 4
  triggers:
    - type: prometheus
      metadata:
        serverAddress: http://prometheus-service.ets-intraday-qe.svc.cluster.local:9091
        metricName: container_cpu_usage_seconds_total
        threshold: "75"
        query: sum(rate(container_cpu_usage_seconds_total{container="query-engine"}[1m]))/(count(container_cpu_usage_seconds_total{container="query-engine"}) * 12) * 100

    - type: prometheus
      metadata:
        serverAddress: http://prometheus-service.ets-intraday-qe.svc.cluster.local:9091
        metricName: task_queue_size
        threshold: "20"
        query: max(task_queue_size)

#https://keda.sh/docs/2.15/scalers/aws-cloudwatch/
#    - type: aws-cloudwatch
#      metadata:
#        namespace: TEST/KEDA
#        dimensionName: InstanceId
#        dimensionValue: i-12345abcdf
#        metricName: TestKedaMetric
#        targetMetricValue: "5"
#        minMetricValue: "0"
#        awsRegion: "us-east-1"
#        # Optional: Collection Time
#        metricCollectionTime: "180" # default 300
#        # Optional: Metric Statistic
#        metricStat: "Average" # default "Average"
#        # Optional: Metric Statistic Period,  must be a value supported by Cloudwatch (1, 5, 10, 30, or a multiple of 60)
#        metricStatPeriod: "60" # default 300
#        # Optional: Metric EndTime Offset
#        metricEndTimeOffset: "0" # default 0
#      authenticationRef:
#        name: keda-trigger-auth-aws-credentials
#---
#apiVersion: keda.sh/v1alpha1
#kind: TriggerAuthentication
#metadata:
#  name: keda-trigger-auth-aws-credentials
#  namespace: default
#spec:
#  podIdentity:
#    provider: aws
```

Prometheusä¸èƒ½é€šè¿‡scaleTargetRefæ¥æŠ“æŒ‡å®šçš„podçš„metric, ä½†å®ƒä¹Ÿæ˜¯ä»kubeletå†…åµŒçš„cadvisoré‡Œé¢æ‹¿åˆ°çš„metric

[https://github.com/prometheus/prometheus/blob/main/documentation/examples/prometheus-kubernetes.yml](https://github.com/prometheus/prometheus/blob/main/documentation/examples/prometheus-kubernetes.yml)

### triggers 

é‡Œé¢çš„å¤šä¸ªæ¡ä»¶æ˜¯orçš„é€»è¾‘ï¼Œæ‰©å®¹æ˜¯ ORï¼Œç¼©å®¹æ˜¯ ANDã€‚

[https://keda.sh/docs/2.17/reference/faq/#best-practices](https://keda.sh/docs/2.17/reference/faq/#best-practices)

### stabilizationWindowSeconds

ä¸¾ä¸ªä¾‹å­ï¼šscaleDown çš„ç¨³å®šçª—å£

```
scaleDown:
Â Â stabilizationWindowSeconds:Â 300
```

- è¡¨ç¤ºåœ¨è¿‡å»Â **5 åˆ†é’Ÿï¼ˆ300 ç§’ï¼‰**Â å†…ï¼ŒHPA ä¼šè®°å½•æ‰€æœ‰çš„ç¼©å®¹å»ºè®®ã€‚

- å®ƒä¸ä¼šç«‹åˆ»æ‰§è¡Œæœ€æ–°çš„ç¼©å®¹å»ºè®®ï¼Œè€Œæ˜¯ä»è¿™ 5 åˆ†é’Ÿå†…çš„å»ºè®®ä¸­é€‰æ‹©Â **æœ€å°çš„å‰¯æœ¬æ•°**Â æ¥æ‰§è¡Œã€‚

ğŸ§ª ç¤ºä¾‹åœºæ™¯

å‡è®¾å½“å‰å‰¯æœ¬æ•°æ˜¯ 4ï¼Œè¿‡å» 5 åˆ†é’Ÿå†… Prometheus æŒ‡æ ‡æ³¢åŠ¨ï¼ŒHPA ç»™å‡ºçš„ç¼©å®¹å»ºè®®å¦‚ä¸‹ï¼š

| æ—¶é—´ç‚¹ | æ¨èå‰¯æœ¬æ•° | 
| -- | -- |
| T-5min | 3 | 
| T-4min | 2 | 
| T-3min | 3 | 
| T-2min | 2 | 
| T-1min | 3 | 


- å¦‚æœæ²¡æœ‰ç¨³å®šçª—å£ï¼ŒHPA å¯èƒ½ä¼šåœ¨æ¯æ¬¡æŒ‡æ ‡ä¸‹é™æ—¶å°±ç«‹åˆ»ç¼©å®¹ï¼Œé€ æˆé¢‘ç¹æ³¢åŠ¨ã€‚

- æœ‰äº†Â stabilizationWindowSeconds: 300ï¼ŒHPA ä¼šä»è¿‡å» 5 åˆ†é’Ÿçš„å»ºè®®ä¸­é€‰æ‹©Â **æœ€å°å€¼ 2**ï¼Œç„¶åå†å†³å®šæ˜¯å¦ç¼©å®¹åˆ° 2ã€‚

å†ä¸¾ä¸ªä¾‹å­ï¼šscaleUp çš„ç¨³å®šçª—å£

```
scaleUp:
Â Â stabilizationWindowSeconds:Â 60
```

- è¡¨ç¤ºåœ¨è¿‡å»Â **1 åˆ†é’Ÿ**Â å†…ï¼ŒHPA ä¼šè®°å½•æ‰€æœ‰çš„æ‰©å®¹å»ºè®®ã€‚

- å®ƒä¼šé€‰æ‹©è¿™æ®µæ—¶é—´å†…çš„Â **æœ€å¤§æ¨èå‰¯æœ¬æ•°**Â æ¥æ‰©å®¹ã€‚

ğŸ§ª ç¤ºä¾‹åœºæ™¯

å½“å‰å‰¯æœ¬æ•°æ˜¯ 2ï¼Œè¿‡å» 1 åˆ†é’Ÿå†…çš„æ‰©å®¹å»ºè®®å¦‚ä¸‹ï¼š

| æ—¶é—´ç‚¹ | æ¨èå‰¯æœ¬æ•° | 
| -- | -- |
| T-60s | 3 | 
| T-30s | 4 | 
| T-10s | 3 | 


- HPA ä¼šé€‰æ‹©æœ€å¤§å€¼Â **4**ï¼Œç„¶åæ‰©å®¹åˆ° 4ã€‚

âœ… æ€»ç»“

| ç±»å‹ | stabilizationWindowSeconds | ä½œç”¨æ–¹å¼ | 
| -- | -- | -- |
| scaleUp | 60 ç§’ | å–è¿‡å» 60 ç§’å†…çš„æœ€å¤§æ¨èå€¼ | 
| scaleDown | 300 ç§’ | å–è¿‡å» 300 ç§’å†…çš„æœ€å°æ¨èå€¼ | 


è¿™ä¸ªæœºåˆ¶çš„å¥½å¤„æ˜¯ï¼š

- é¿å…å› æŒ‡æ ‡çŸ­æ—¶é—´æ³¢åŠ¨è€Œé¢‘ç¹æ‰©ç¼©å®¹ã€‚

- æé«˜ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œèµ„æºä½¿ç”¨æ•ˆç‡ã€‚

### Ploicyçš„ç”¨å¤„

åŸºæœ¬æ˜¯é€ä¼ çš„[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#configurable-scaling-behavior](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#configurable-scaling-behavior)

[https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/#HorizontalPodAutoscalerSpec](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/#HorizontalPodAutoscalerSpec)

selectPolicyä¸è®ºscaleupè¿˜æ˜¯down, é»˜è®¤å€¼éƒ½æ˜¯max

ğŸ§ª å‡è®¾å½“å‰å‰¯æœ¬æ•°æ˜¯ 5ï¼š

ğŸ”½ ç¼©å®¹ç­–ç•¥ï¼ˆscaleDownï¼‰

- Percent: 20Â â†’ æœ€å¤šç¼©å®¹ 20%ï¼š5Ã—20%=15Ã—20%=1ï¼ˆå‘ä¸‹å–æ•´ï¼‰

- Pods: 1Â â†’ æœ€å¤šç¼©å®¹ 1 ä¸ª Pod

- periodSeconds: 300Â â†’ æ¯ 5 åˆ†é’Ÿæœ€å¤šæ‰§è¡Œä¸€æ¬¡ç¼©å®¹

**æœ€ç»ˆæ•ˆæœ**ï¼šæ¯ 5 åˆ†é’Ÿæœ€å¤šç¼©å®¹ 1 ä¸ª Podï¼ˆå› ä¸ºä¸¤ä¸ªç­–ç•¥éƒ½é™åˆ¶ä¸º 1ï¼‰

ğŸ”¼ æ‰©å®¹ç­–ç•¥ï¼ˆscaleUpï¼‰

- Percent: 20Â â†’ æœ€å¤šæ‰©å®¹ 1 ä¸ª Podï¼ˆ5Ã—20%=15Ã—20%=1ï¼‰

- Pods: 1Â â†’ æœ€å¤šæ‰©å®¹ 1 ä¸ª Pod

- selectPolicy: MaxÂ â†’ é€‰æ‹©ä¸¤ä¸ªç­–ç•¥ä¸­å…è®¸çš„æœ€å¤§å€¼

**æœ€ç»ˆæ•ˆæœ**ï¼šæ¯æ¬¡æ‰©å®¹æœ€å¤šå¢åŠ  1 ä¸ª Pod

æ•ˆæœä¸¾ä¾‹ï¼Œ é»˜è®¤scale desiredçš„å»ºè®®æ˜¯15ç§’æ›´æ–°ä¸€æ¬¡ã€‚

![](images/WEBRESOURCE58f85927d828ebb06f59d67ed8bd8311image.png)

åœ¨ HPA ä¸­ï¼ŒstabilizationWindowSecondsÂ ä¼šè®©æ§åˆ¶å™¨åœ¨æ‰©å®¹æ—¶å‚è€ƒè¿‡å»çª—å£å†…çš„æœ€å°æ¨èå‰¯æœ¬æ•°ï¼Œåœ¨ç¼©å®¹æ—¶å‚è€ƒæœ€å¤§æ¨èå‰¯æœ¬æ•°ï¼Œä»¥æ­¤æ¥ç¨³å®šæ‰©ç¼©å®¹è¡Œä¸ºã€‚

[https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/podautoscaler/horizontal.go#L1134](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/podautoscaler/horizontal.go#L1134)

```python
# hpa_scaleup_bucket_switch.py
# -*- coding: utf-8 -*-
"""
HPA scale-up simulation & plotting:
- Sync period = 15s (one scale action at most per sync)
- scaleUp policies (selectPolicy=Max):
    * Percent = 10% per 60s
    * Pods    = +1   per 90s
  Each policy uses its own "bucket" (quota) within its periodSeconds window.
  The bucket does NOT grow inside the window; it resets when the window expires.
  This produces realistic policy switching rather than always picking percent.
- Compare scaleUp.stabilizationWindowSeconds = 0s vs 300s
- desired replicas fixed to 20, start replicas 10
- Step lines drawn with explicit vertical jumps at t, arrow tip placed at the
  midpoint of each vertical jump (precise alignment).

Dependencies: numpy, matplotlib
> pip install numpy matplotlib
"""

import math
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator, FuncFormatter

# -----------------------------
# Parameters (tweak as needed)
# -----------------------------
SYNC_S = 15          # HPA sync period in seconds
START = 10           # initial replicas
DESIRED = 20         # desired replicas (kept constant for this demo)

# scaleUp policies
PERCENT_VALUE   = 10   # 10%
PERCENT_PERIOD  = 60   # seconds
PODS_VALUE      = 1    # +1
PODS_PERIOD     = 90   # seconds

# Compare two scaleUp windows
WIN_A = 0            # seconds
WIN_B = 300          # seconds

# Total simulation length
T_TOTAL = 15 * 60    # 15 minutes in seconds

# -----------------------------
# Simulation with per-policy "buckets"
# -----------------------------
def simulate_bucket(scaleup_window_seconds: int):
    """
    Simulate HPA scale-up using independent per-policy buckets (quota).
    - For each policy, we compute 'remaining allowance' inside its window.
    - selectPolicy=Max: among policies with remaining allowance > 0,
      pick the one with larger allowance (tie-breaker: one with sooner expiry).
    - One scaling action per sync cycle.
    Returns:
      X_path (np.array seconds), Y_path (replicas with vertical jumps),
      events: list[(t, old, new, policy_str)]
    """
    cur = START

    # Percent bucket
    p_bucket_start = None  # window start timestamp
    p_used = 0             # used replicas within current window
    p_baseline = None      # replicas at first scale within current 60s window

    # Pods bucket
    n_bucket_start = None
    n_used = 0

    # For drawing exact vertical jumps at x=t
    X_path, Y_path = [], []
    events = []  # (t, old, new, policy_str)

    for t in range(0, T_TOTAL + 1, SYNC_S):
        old = cur

        # record plateau value BEFORE any potential scaling at this tick
        X_path.append(t)
        Y_path.append(old)

        # gate by stabilization window and desired
        if cur < DESIRED and t >= scaleup_window_seconds:

            # Reset percent bucket if window expired; freeze baseline for new window
            if p_bucket_start is None or (t - p_bucket_start) >= PERCENT_PERIOD:
                p_bucket_start = t
                p_used = 0
                p_baseline = cur  # <= the baseline is frozen for this 60s window

            # Reset pods bucket if window expired
            if n_bucket_start is None or (t - n_bucket_start) >= PODS_PERIOD:
                n_bucket_start = t
                n_used = 0

            # Remaining allowances
            p_limit = max(1, math.ceil(p_baseline * PERCENT_VALUE / 100.0))
            p_allow = max(0, p_limit - p_used)

            n_limit = PODS_VALUE
            n_allow = max(0, n_limit - n_used)

            # Collect candidates (allow > 0)
            candidates = []
            if p_allow > 0:
                time_to_expire_p = p_bucket_start + PERCENT_PERIOD - t
                candidates.append((p_allow, "10%/60s", time_to_expire_p))
            if n_allow > 0:
                time_to_expire_n = n_bucket_start + PODS_PERIOD - t
                candidates.append((n_allow, "+1/90s", time_to_expire_n))

            if candidates:
                # selectPolicy=Max: pick larger allowance; tieâ†’window that expires sooner
                candidates.sort(key=lambda z: (-z[0], z[2]))
                allow, policy, _ = candidates[0]
                inc = min(allow, DESIRED - cur)
                if inc > 0:
                    cur += inc
                    # Update the chosen bucket's usage
                    if policy == "10%/60s":
                        p_used += inc
                    else:
                        n_used += inc

                    # Insert a duplicate x=t with AFTER value -> visible vertical jump
                    X_path.append(t)
                    Y_path.append(cur)
                    events.append((t, old, cur, policy))

    return np.array(X_path), np.array(Y_path), events


# Run both scenarios
X0, Y0, E0 = simulate_bucket(WIN_A)
X300, Y300, E300 = simulate_bucket(WIN_B)

# -----------------------------
# Plot
# -----------------------------
plt.figure(figsize=(13.5, 6.7))

# Step curves (we already inserted vertical jumps at exact t)
plt.plot(X0 / 60.0, Y0, drawstyle="steps-post",
         label=f"scaleUp window = {WIN_A}s", linewidth=2, color="#1f77b4")
plt.plot(X300 / 60.0, Y300, drawstyle="steps-post",
         label=f"scaleUp window = {WIN_B}s", linewidth=2, color="#ff7f0e")

# Desired & Stabilization area
plt.hlines(DESIRED, 0, T_TOTAL / 60.0, colors="crimson",
           linestyles="--", linewidth=2, label=f"desired = {DESIRED} (fixed)")
plt.axvspan(0, WIN_B / 60.0, color="orange", alpha=0.08,
            label=f"{WIN_B}s stabilization window")

# Annotate: arrow tip at the midpoint of the vertical jump
COLOR = {"10%/60s": "#2ca02c", "+1/90s": "#d62728"}

def annotate(events, side="right", y_jitter=0.0):
    for t, old, new, pol in events:
        x = t / 60.0
        y_mid = (old + new) / 2.0 + y_jitter
        c = COLOR[pol]
        # small mid-point marker (sanity check)
        plt.scatter([x], [y_mid], s=22, facecolor="white", edgecolor=c, zorder=4)
        # place label to right/left to avoid overlap
        xytext = (10, 0) if side == "right" else (-10, 0)
        ha     = "left"  if side == "right" else "right"
        plt.annotate(
            f"{int(new)} â† {pol}",
            xy=(x, y_mid), xytext=xytext, textcoords="offset points",
            ha=ha, va="center", fontsize=9, color=c,
            arrowprops=dict(arrowstyle="->", color=c, lw=0.9),
        )

# åˆ†åˆ«æŠŠä¸¤æ¡æ›²çº¿çš„æ ‡æ³¨æ”¾åœ¨ä¸åŒä¾§ï¼ˆå¹¶å¾®æŠ– yï¼‰ï¼Œé¿å…é®æŒ¡
annotate(E0,   side="right", y_jitter=+0.00)  # window=0s
annotate(E300, side="left",  y_jitter=-0.06)  # window=300s

# X è½´ï¼šmm:ssï¼ˆä¸»åˆ»åº¦ 1 åˆ†é’Ÿã€æ¬¡åˆ»åº¦ 15 ç§’ï¼‰
ax = plt.gca()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.xaxis.set_minor_locator(MultipleLocator(0.25))  # 0.25 min = 15s
def fmt_mmss(x, pos):
    total_sec = int(round(x * 60))
    m, s = divmod(total_sec, 60)
    return f"{m:02d}:{s:02d}"
ax.xaxis.set_major_formatter(FuncFormatter(fmt_mmss))

plt.xlabel("Time (mm:ss)")
plt.ylabel("Replicas")
plt.title(
    "HPA scale-up (15s sync) with per-policy buckets (policy switching)\n"
    "Policies: 10%/60s & +1/90s (selectPolicy=Max); start=10, desired=20\n"
    "Arrows point to midpoint of vertical jump at x=t (exact alignment)"
)
plt.xlim(0, T_TOTAL / 60.0)
plt.ylim(9, 22)
plt.grid(True, which="major", axis="x", alpha=0.35)
plt.grid(True, which="minor", axis="x", alpha=0.15)
plt.grid(True, which="major", axis="y", alpha=0.25)
plt.legend(loc="lower right")
plt.tight_layout()

out_path = "hpa_scaleup_bucket_switch.png"
plt.savefig(out_path, dpi=240)
print(f"Saved figure: {out_path}")

# -----------------------------
# Print events (optional)
# -----------------------------
def dump(events, title):
    print("\n" + title)
    for t, old, new, pol in events:
        m, s = divmod(t, 60)
        print(f"{m:02d}:{s:02d}  {old}->{new}  {pol}")

dump(E0,   f"Events (scaleUp window = {WIN_A}s)")
dump(E300, f"Events (scaleUp window = {WIN_B}s)")

```