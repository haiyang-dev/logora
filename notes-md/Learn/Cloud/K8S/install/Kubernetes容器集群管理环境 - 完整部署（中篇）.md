接着Kubernetes容器集群管理环境 - 完整部署（上篇）继续往下部署：

八、部署master节点

master节点的kube-apiserver、kube-scheduler 和 kube-controller-manager 均以多实例模式运行：kube-scheduler 和 kube-controller-manager 会自动选举产生一个 leader 实例，其它实例处于阻塞模式，当 leader 挂了后，重新选举产生新的 leader，从而保证服务可用性；kube-apiserver 是无状态的，需要通过 kube-nginx 进行代理访问，从而保证服务可用性；下面部署命令均在k8s-master01节点上执行，然后远程分发文件和执行命令。

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16 | 下载最新版本二进制文件<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# wget https://dl.k8s.io/v1.14.2/kubernetes-server-linux-amd64.tar.gz<br>[root@k8s-master01 work]\# tar -xzvf kubernetes-server-linux-amd64.tar.gz<br>[root@k8s-master01 work]\# cd kubernetes<br>[root@k8s-master01 work]\# tar -xzvf  kubernetes-src.tar.gz<br> <br>将二进制文件拷贝到所有 master 节点：<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp kubernetes/server/bin/{apiextensions-apiserver,cloud-controller-manager,kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,kubeadm,kubectl,kubelet,mounter} root@${node\_master\_ip}:/opt/k8s/bin/<br>    ssh root@${node\_master\_ip} "chmod +x /opt/k8s/bin/\*"<br>  done |


8.1 - 部署高可用 kube-apiserver 集群

这里部署一个三实例kube-apiserver集群环境，它们通过nginx四层代理进行访问，对外提供一个统一的vip地址，从而保证服务可用性。下面部署命令均在k8s-master01节点上执行，然后远程分发文件和执行命令。

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570 | 1) 创建 kubernetes 证书和私钥<br>创建证书签名请求：<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# cat &gt; kubernetes-csr.json &lt;&lt;EOF<br>{<br>  "CN": "kubernetes",<br>  "hosts": [<br>    "127.0.0.1",<br>    "172.16.60.250",<br>    "172.16.60.241",<br>    "172.16.60.242",<br>    "172.16.60.243",<br>    "${CLUSTER\_KUBERNETES\_SVC\_IP}",<br>    "kubernetes",<br>    "kubernetes.default",<br>    "kubernetes.default.svc",<br>    "kubernetes.default.svc.cluster",<br>    "kubernetes.default.svc.cluster.local"<br>  ],<br>  "key": {<br>    "algo": "rsa",<br>    "size": 2048<br>  },<br>  "names": [<br>    {<br>      "C": "CN",<br>      "ST": "BeiJing",<br>      "L": "BeiJing",<br>      "O": "k8s",<br>      "OU": "4Paradigm"<br>    }<br>  ]<br>}<br>EOF<br>  <br>解释说明：<br>• hosts 字段指定授权使用该证书的 IP 或域名列表，这里列出了 VIP 、apiserver 节点 IP、kubernetes 服务 IP 和域名；<br>• 域名最后字符不能是 .(如不能为 kubernetes.default.svc.cluster.local.)，否则解析时失败，提示：<br>  x509: cannot parse dnsName "kubernetes.default.svc.cluster.local."；<br>• 如果使用非 cluster.local 域名，如 opsnull.com，则需要修改域名列表中的最后两个域名为：kubernetes.default.svc.opsnull、kubernetes.default.svc.opsnull.com<br>• kubernetes 服务 IP 是 apiserver 自动创建的，一般是 --service-cluster-ip-range 参数指定的网段的第一个IP，后续可以通过如下命令获取：<br>  <br>[root@k8s-master01 work]\# kubectl get svc kubernetes<br>The connection to the server 172.16.60.250:8443 was refused - did you specify the right host or port?<br>   <br>上面报错是因为kube-apiserver服务此时没有启动，后续待apiserver服务启动后，以上命令就可以获得了。<br>   <br>生成证书和私钥：<br>[root@k8s-master01 work]\# cfssl gencert -ca=/opt/k8s/work/ca.pem \\<br>  -ca-key=/opt/k8s/work/ca-key.pem \\<br>  -config=/opt/k8s/work/ca-config.json \\<br>  -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes<br>   <br>[root@k8s-master01 work]\# ls kubernetes\*pem<br>kubernetes-key.pem  kubernetes.pem<br>   <br>将生成的证书和私钥文件拷贝到所有 master 节点：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    ssh root@${node\_master\_ip} "mkdir -p /etc/kubernetes/cert"<br>    scp kubernetes\*.pem root@${node\_master\_ip}:/etc/kubernetes/cert/<br>  done<br>   <br>2) 创建加密配置文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# cat &gt; encryption-config.yaml &lt;&lt;EOF<br>kind: EncryptionConfig<br>apiVersion: v1<br>resources:<br>  - resources:<br>      - secrets<br>    providers:<br>      - aescbc:<br>          keys:<br>            - name: key1<br>              secret: ${ENCRYPTION\_KEY}<br>      - identity: {}<br>EOF<br>   <br>将加密配置文件拷贝到 master 节点的 /etc/kubernetes 目录下：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp encryption-config.yaml root@${node\_master\_ip}:/etc/kubernetes/<br>  done<br>   <br>3) 创建审计策略文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# cat &gt; audit-policy.yaml &lt;&lt;EOF<br>apiVersion: audit.k8s.io/v1beta1<br>kind: Policy<br>rules:<br>  \# The following requests were manually identified as high-volume and low-risk, so drop them.<br>  - level: None<br>    resources:<br>      - group: ""<br>        resources:<br>          - endpoints<br>          - services<br>          - services/status<br>    users:<br>      - 'system:kube-proxy'<br>    verbs:<br>      - watch<br>   <br>  - level: None<br>    resources:<br>      - group: ""<br>        resources:<br>          - nodes<br>          - nodes/status<br>    userGroups:<br>      - 'system:nodes'<br>    verbs:<br>      - get<br>   <br>  - level: None<br>    namespaces:<br>      - kube-system<br>    resources:<br>      - group: ""<br>        resources:<br>          - endpoints<br>    users:<br>      - 'system:kube-controller-manager'<br>      - 'system:kube-scheduler'<br>      - 'system:serviceaccount:kube-system:endpoint-controller'<br>    verbs:<br>      - get<br>      - update<br>   <br>  - level: None<br>    resources:<br>      - group: ""<br>        resources:<br>          - namespaces<br>          - namespaces/status<br>          - namespaces/finalize<br>    users:<br>      - 'system:apiserver'<br>    verbs:<br>      - get<br>   <br>  \# Don't log HPA fetching metrics.<br>  - level: None<br>    resources:<br>      - group: metrics.k8s.io<br>    users:<br>      - 'system:kube-controller-manager'<br>    verbs:<br>      - get<br>      - list<br>   <br>  \# Don't log these read-only URLs.<br>  - level: None<br>    nonResourceURLs:<br>      - '/healthz\*'<br>      - /version<br>      - '/swagger\*'<br>   <br>  \# Don't log events requests.<br>  - level: None<br>    resources:<br>      - group: ""<br>        resources:<br>          - events<br>   <br>  \# node and pod status calls from nodes are high-volume and can be large, don't log responses for expected updates from nodes<br>  - level: Request<br>    omitStages:<br>      - RequestReceived<br>    resources:<br>      - group: ""<br>        resources:<br>          - nodes/status<br>          - pods/status<br>    users:<br>      - kubelet<br>      - 'system:node-problem-detector'<br>      - 'system:serviceaccount:kube-system:node-problem-detector'<br>    verbs:<br>      - update<br>      - patch<br>   <br>  - level: Request<br>    omitStages:<br>      - RequestReceived<br>    resources:<br>      - group: ""<br>        resources:<br>          - nodes/status<br>          - pods/status<br>    userGroups:<br>      - 'system:nodes'<br>    verbs:<br>      - update<br>      - patch<br>   <br>  \# deletecollection calls can be large, don't log responses for expected namespace deletions<br>  - level: Request<br>    omitStages:<br>      - RequestReceived<br>    users:<br>      - 'system:serviceaccount:kube-system:namespace-controller'<br>    verbs:<br>      - deletecollection<br>   <br>  \# Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data,<br>  \# so only log at the Metadata level.<br>  - level: Metadata<br>    omitStages:<br>      - RequestReceived<br>    resources:<br>      - group: ""<br>        resources:<br>          - secrets<br>          - configmaps<br>      - group: authentication.k8s.io<br>        resources:<br>          - tokenreviews<br>  \# Get repsonses can be large; skip them.<br>  - level: Request<br>    omitStages:<br>      - RequestReceived<br>    resources:<br>      - group: ""<br>      - group: admissionregistration.k8s.io<br>      - group: apiextensions.k8s.io<br>      - group: apiregistration.k8s.io<br>      - group: apps<br>      - group: authentication.k8s.io<br>      - group: authorization.k8s.io<br>      - group: autoscaling<br>      - group: batch<br>      - group: certificates.k8s.io<br>      - group: extensions<br>      - group: metrics.k8s.io<br>      - group: networking.k8s.io<br>      - group: policy<br>      - group: rbac.authorization.k8s.io<br>      - group: scheduling.k8s.io<br>      - group: settings.k8s.io<br>      - group: storage.k8s.io<br>    verbs:<br>      - get<br>      - list<br>      - watch<br>   <br>  \# Default level for known APIs<br>  - level: RequestResponse<br>    omitStages:<br>      - RequestReceived<br>    resources:<br>      - group: ""<br>      - group: admissionregistration.k8s.io<br>      - group: apiextensions.k8s.io<br>      - group: apiregistration.k8s.io<br>      - group: apps<br>      - group: authentication.k8s.io<br>      - group: authorization.k8s.io<br>      - group: autoscaling<br>      - group: batch<br>      - group: certificates.k8s.io<br>      - group: extensions<br>      - group: metrics.k8s.io<br>      - group: networking.k8s.io<br>      - group: policy<br>      - group: rbac.authorization.k8s.io<br>      - group: scheduling.k8s.io<br>      - group: settings.k8s.io<br>      - group: storage.k8s.io<br>         <br>  \# Default level for all other requests.<br>  - level: Metadata<br>    omitStages:<br>      - RequestReceived<br>EOF<br>   <br>分发审计策略文件：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp audit-policy.yaml root@${node\_master\_ip}:/etc/kubernetes/audit-policy.yaml<br>  done<br>   <br>4) 创建后续访问 metrics-server 使用的证书<br>创建证书签名请求:<br>[root@k8s-master01 work]\# cat &gt; proxy-client-csr.json &lt;&lt;EOF<br>{<br>  "CN": "aggregator",<br>  "hosts": [],<br>  "key": {<br>    "algo": "rsa",<br>    "size": 2048<br>  },<br>  "names": [<br>    {<br>      "C": "CN",<br>      "ST": "BeiJing",<br>      "L": "BeiJing",<br>      "O": "k8s",<br>      "OU": "4Paradigm"<br>    }<br>  ]<br>}<br>EOF<br>   <br>CN 名称为 aggregator，需要与 metrics-server 的 --requestheader-allowed-names 参数配置一致，否则访问会被 metrics-server 拒绝；<br>   <br>生成证书和私钥：<br>[root@k8s-master01 work]\# cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \\<br>  -ca-key=/etc/kubernetes/cert/ca-key.pem  \\<br>  -config=/etc/kubernetes/cert/ca-config.json  \\<br>  -profile=kubernetes proxy-client-csr.json | cfssljson -bare proxy-client<br>   <br>[root@k8s-master01 work]\# ls proxy-client\*.pem<br>proxy-client-key.pem  proxy-client.pem<br>   <br>将生成的证书和私钥文件拷贝到所有 master 节点：<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp proxy-client\*.pem root@${node\_master\_ip}:/etc/kubernetes/cert/<br>  done<br>   <br>5) 创建 kube-apiserver systemd unit 模板文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# cat &gt; kube-apiserver.service.template &lt;&lt;EOF<br>[Unit]<br>Description=Kubernetes API Server<br>Documentation=https://github.com/GoogleCloudPlatform/kubernetes<br>After=network.target<br>   <br>[Service]<br>WorkingDirectory=${K8S\_DIR}/kube-apiserver<br>ExecStart=/opt/k8s/bin/kube-apiserver \\\\<br>  --advertise-address=\#\#NODE\_MASTER\_IP\#\# \\\\<br>  --default-not-ready-toleration-seconds=360 \\\\<br>  --default-unreachable-toleration-seconds=360 \\\\<br>  --feature-gates=DynamicAuditing=true \\\\<br>  --max-mutating-requests-inflight=2000 \\\\<br>  --max-requests-inflight=4000 \\\\<br>  --default-watch-cache-size=200 \\\\<br>  --delete-collection-workers=2 \\\\<br>  --encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\\\<br>  --etcd-cafile=/etc/kubernetes/cert/ca.pem \\\\<br>  --etcd-certfile=/etc/kubernetes/cert/kubernetes.pem \\\\<br>  --etcd-keyfile=/etc/kubernetes/cert/kubernetes-key.pem \\\\<br>  --etcd-servers=${ETCD\_ENDPOINTS} \\\\<br>  --bind-address=\#\#NODE\_MASTER\_IP\#\# \\\\<br>  --secure-port=6443 \\\\<br>  --tls-cert-file=/etc/kubernetes/cert/kubernetes.pem \\\\<br>  --tls-private-key-file=/etc/kubernetes/cert/kubernetes-key.pem \\\\<br>  --insecure-port=0 \\\\<br>  --audit-dynamic-configuration \\\\<br>  --audit-log-maxage=15 \\\\<br>  --audit-log-maxbackup=3 \\\\<br>  --audit-log-maxsize=100 \\\\<br>  --audit-log-mode=batch \\\\<br>  --audit-log-truncate-enabled \\\\<br>  --audit-log-batch-buffer-size=20000 \\\\<br>  --audit-log-batch-max-size=2 \\\\<br>  --audit-log-path=${K8S\_DIR}/kube-apiserver/audit.log \\\\<br>  --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\\\<br>  --profiling \\\\<br>  --anonymous-auth=false \\\\<br>  --client-ca-file=/etc/kubernetes/cert/ca.pem \\\\<br>  --enable-bootstrap-token-auth \\\\<br>  --requestheader-allowed-names="" \\\\<br>  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\\\<br>  --requestheader-extra-headers-prefix="X-Remote-Extra-" \\\\<br>  --requestheader-group-headers=X-Remote-Group \\\\<br>  --requestheader-username-headers=X-Remote-User \\\\<br>  --service-account-key-file=/etc/kubernetes/cert/ca.pem \\\\<br>  --authorization-mode=Node,RBAC \\\\<br>  --runtime-config=api/all=true \\\\<br>  --enable-admission-plugins=NodeRestriction \\\\<br>  --allow-privileged=true \\\\<br>  --apiserver-count=3 \\\\<br>  --event-ttl=168h \\\\<br>  --kubelet-certificate-authority=/etc/kubernetes/cert/ca.pem \\\\<br>  --kubelet-client-certificate=/etc/kubernetes/cert/kubernetes.pem \\\\<br>  --kubelet-client-key=/etc/kubernetes/cert/kubernetes-key.pem \\\\<br>  --kubelet-https=true \\\\<br>  --kubelet-timeout=10s \\\\<br>  --proxy-client-cert-file=/etc/kubernetes/cert/proxy-client.pem \\\\<br>  --proxy-client-key-file=/etc/kubernetes/cert/proxy-client-key.pem \\\\<br>  --service-cluster-ip-range=${SERVICE\_CIDR} \\\\<br>  --service-node-port-range=${NODE\_PORT\_RANGE} \\\\<br>  --logtostderr=true \\\\<br>  --enable-aggregator-routing=true \\\\<br>  --v=2<br>Restart=on-failure<br>RestartSec=10<br>Type=notify<br>LimitNOFILE=65536<br>   <br>[Install]<br>WantedBy=multi-user.target<br>EOF<br>   <br>解释说明：<br>--advertise-address：apiserver 对外通告的 IP（kubernetes 服务后端节点 IP）；<br>--default-\*-toleration-seconds：设置节点异常相关的阈值；<br>--max-\*-requests-inflight：请求相关的最大阈值；<br>--etcd-\*：访问 etcd 的证书和 etcd 服务器地址；<br>--experimental-encryption-provider-config：指定用于加密 etcd 中 secret 的配置；<br>--bind-address： https 监听的 IP，不能为 127.0.0.1，否则外界不能访问它的安全端口 6443；<br>--secret-port：https 监听端口；<br>--insecure-port=0：关闭监听 http 非安全端口(8080)；<br>--tls-\*-file：指定 apiserver 使用的证书、私钥和 CA 文件；<br>--audit-\*：配置审计策略和审计日志文件相关的参数；<br>--client-ca-file：验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy 等)请求所带的证书；<br>--enable-bootstrap-token-auth：启用 kubelet bootstrap 的 token 认证；<br>--requestheader-\*：kube-apiserver 的 aggregator layer 相关的配置参数，proxy-client &amp; HPA 需要使用；<br>--requestheader-client-ca-file：用于签名 --proxy-client-cert-file 和 --proxy-client-key-file 指定的证书；在启用了 metric aggregator 时使用；<br>如果 --requestheader-allowed-names 不为空，则--proxy-client-cert-file 证书的 CN 必须位于 allowed-names 中，默认为 aggregator;<br>--service-account-key-file：签名 ServiceAccount Token 的公钥文件，kube-controller-manager 的 --service-account-private-key-file 指定私钥文件，两者配对使用；<br>--runtime-config=api/all=true： 启用所有版本的 APIs，如 autoscaling/v2alpha1；<br>--authorization-mode=Node,RBAC、--anonymous-auth=false： 开启 Node 和 RBAC 授权模式，拒绝未授权的请求；<br>--enable-admission-plugins：启用一些默认关闭的 plugins；<br>--allow-privileged：运行执行 privileged 权限的容器；<br>--apiserver-count=3：指定 apiserver 实例的数量；<br>--event-ttl：指定 events 的保存时间；<br>--kubelet-\*：如果指定，则使用 https 访问 kubelet APIs；需要为证书对应的用户(上面 kubernetes\*.pem 证书的用户为 kubernetes) 用户定义 RBAC 规则，否则访问 kubelet API 时提示未授权；<br>--proxy-client-\*：apiserver 访问 metrics-server 使用的证书；<br>--service-cluster-ip-range： 指定 Service Cluster IP 地址段；<br>--service-node-port-range： 指定 NodePort 的端口范围；<br>   <br>注意：<br>如果kube-apiserver机器没有运行 kube-proxy，则需要添加 --enable-aggregator-routing=true 参数（这里master节点没有作为node节点使用，故没有运行kube-proxy，需要加这个参数）<br>requestheader-client-ca-file 指定的 CA 证书，必须具有 client auth and server auth！！<br>   <br>为各节点创建和分发 kube-apiserver systemd unit 文件<br>替换模板文件中的变量，为各节点生成 systemd unit 文件：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for (( i=0; i &lt; 3; i++ ))<br>  do<br>    sed -e "s/\#\#NODE\_MASTER\_NAME\#\#/${NODE\_MASTER\_NAMES[i]}/" -e "s/\#\#NODE\_MASTER\_IP\#\#/${NODE\_MASTER\_IPS[i]}/" kube-apiserver.service.template &gt; kube-apiserver-${NODE\_MASTER\_IPS[i]}.service<br>  done<br>   <br>其中：NODE\_NAMES 和 NODE\_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；<br>   <br>[root@k8s-master01 work]\# ll kube-apiserver\*.service<br>-rw-r--r-- 1 root root 2718 Jun 18 10:38 kube-apiserver-172.16.60.241.service<br>-rw-r--r-- 1 root root 2718 Jun 18 10:38 kube-apiserver-172.16.60.242.service<br>-rw-r--r-- 1 root root 2718 Jun 18 10:38 kube-apiserver-172.16.60.243.service<br>   <br>分发生成的 systemd unit 文件, 文件重命名为 kube-apiserver.service;<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp kube-apiserver-${node\_master\_ip}.service root@${node\_master\_ip}:/etc/systemd/system/kube-apiserver.service<br>  done<br>   <br>6) 启动 kube-apiserver 服务<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    ssh root@${node\_master\_ip} "mkdir -p ${K8S\_DIR}/kube-apiserver"<br>    ssh root@${node\_master\_ip} "systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl restart kube-apiserver"<br>  done<br>   <br>注意：启动服务前必须先创建工作目录；<br>   <br>检查 kube-apiserver 运行状态<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    ssh root@${node\_master\_ip} "systemctl status kube-apiserver |grep 'Active:'"<br>  done<br>   <br>预期输出：<br>&gt;&gt;&gt; 172.16.60.241<br>   Active: active (running) since Tue 2019-06-18 10:42:42 CST; 1min 6s ago<br>&gt;&gt;&gt; 172.16.60.242<br>   Active: active (running) since Tue 2019-06-18 10:42:47 CST; 1min 2s ago<br>&gt;&gt;&gt; 172.16.60.243<br>   Active: active (running) since Tue 2019-06-18 10:42:51 CST; 58s ago<br>   <br>确保状态为 active (running)，否则查看日志，确认原因（journalctl -u kube-apiserver）<br>   <br>7）打印 kube-apiserver 写入 etcd 的数据<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# ETCDCTL\_API=3 etcdctl \\<br>    --endpoints=${ETCD\_ENDPOINTS} \\<br>    --cacert=/opt/k8s/work/ca.pem \\<br>    --cert=/opt/k8s/work/etcd.pem \\<br>    --key=/opt/k8s/work/etcd-key.pem \\<br>    get /registry/ --prefix --keys-only<br>   <br>预期会打印出很多写入到etcd中的数据信息<br>   <br>8）检查集群信息<br>[root@k8s-master01 work]\# kubectl cluster-info<br>Kubernetes master is running at https://172.16.60.250:8443<br>To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.<br>   <br>[root@k8s-master01 work]\# kubectl get all --all-namespaces<br>NAMESPACE   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE<br>default     service/kubernetes   ClusterIP   10.254.0.1   &lt;none&gt;        443/TCP   8m25s<br>   <br>查看集群状态信息<br>[root@k8s-master01 work]\# kubectl get componentstatuses            \#或者执行命令"kubectl get cs"<br>NAME                 STATUS      MESSAGE                                                                                     ERROR<br>controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused<br>scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused<br>etcd-0               Healthy     {"health":"true"}                                                                        <br>etcd-2               Healthy     {"health":"true"}                                                                        <br>etcd-1               Healthy     {"health":"true"}<br>   <br>controller-managerhe 和 schedule状态为Unhealthy，是因为此时还没有部署这两个组件，待后续部署好之后再查看~<br>   <br>这里注意：<br>-&gt; 如果执行 kubectl 命令式时输出如下错误信息，则说明使用的 ~/.kube/config 文件不对，请切换到正确的账户后再执行该命令：<br>   The connection to the server localhost:8080 was refused - did you specify the right host or port?<br>-&gt; 执行 kubectl get componentstatuses 命令时，apiserver 默认向 127.0.0.1 发送请求。当 controller-manager、scheduler 以集群模式运行时，有可能和kube-apiserver<br>   不在一台机器上，这时 controller-manager 或 scheduler 的状态为 Unhealthy，但实际上它们工作正常。<br>   <br>9) 检查 kube-apiserver 监听的端口<br>[root@k8s-master01 work]\# netstat -lnpt|grep kube<br>tcp        0      0 172.16.60.241:6443      0.0.0.0:\*               LISTEN      15516/kube-apiserve<br>   <br>需要注意：<br>6443: 接收 https 请求的安全端口，对所有请求做认证和授权；<br>由于关闭了非安全端口，故没有监听 8080；<br>   <br>10）授予 kube-apiserver 访问 kubelet API 的权限<br>在执行 kubectl exec、run、logs 等命令时，apiserver 会将请求转发到 kubelet 的 https 端口。<br>这里定义 RBAC 规则，授权 apiserver 使用的证书（kubernetes.pem）用户名（CN：kuberntes）访问 kubelet API 的权限：<br>   <br>[root@k8s-master01 work]\# kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes<br> <br>11）查看kube-apiserver输出的metrics<br>需要用到根证书<br> <br>使用nginx的代理端口获取metrics<br>[root@k8s-master01 work]\# curl -s --cacert /opt/k8s/work/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://172.16.60.250:8443/metrics|head<br>\# HELP APIServiceOpenAPIAggregationControllerQueue1\_adds (Deprecated) Total number of adds handled by workqueue: APIServiceOpenAPIAggregationControllerQueue1<br>\# TYPE APIServiceOpenAPIAggregationControllerQueue1\_adds counter<br>APIServiceOpenAPIAggregationControllerQueue1\_adds 12194<br>\# HELP APIServiceOpenAPIAggregationControllerQueue1\_depth (Deprecated) Current depth of workqueue: APIServiceOpenAPIAggregationControllerQueue1<br>\# TYPE APIServiceOpenAPIAggregationControllerQueue1\_depth gauge<br>APIServiceOpenAPIAggregationControllerQueue1\_depth 0<br>\# HELP APIServiceOpenAPIAggregationControllerQueue1\_longest\_running\_processor\_microseconds (Deprecated) How many microseconds has the longest running processor for APIServiceOpenAPIAggregationControllerQueue1 been running.<br>\# TYPE APIServiceOpenAPIAggregationControllerQueue1\_longest\_running\_processor\_microseconds gauge<br>APIServiceOpenAPIAggregationControllerQueue1\_longest\_running\_processor\_microseconds 0<br>\# HELP APIServiceOpenAPIAggregationControllerQueue1\_queue\_latency (Deprecated) How long an item stays in workqueueAPIServiceOpenAPIAggregationControllerQueue1 before being requested.<br> <br>直接使用kube-apiserver节点端口获取metrics<br>[root@k8s-master01 work]\# curl -s --cacert /opt/k8s/work/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://172.16.60.241:6443/metrics|head<br>[root@k8s-master01 work]\# curl -s --cacert /opt/k8s/work/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://172.16.60.242:6443/metrics|head<br>[root@k8s-master01 work]\# curl -s --cacert /opt/k8s/work/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://172.16.60.243:6443/metrics|head |


8.2 - 部署高可用 kube-controller-manager 集群

该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用时，阻塞的节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。为保证通信安全，本文档先生成 x509 证书和私钥，kube-controller-manager 在如下两种情况下使用该证书：与 kube-apiserver 的安全端口通信; 在安全端口(https，10252) 输出 prometheus 格式的 metrics；下面部署命令均在k8s-master01节点上执行，然后远程分发文件和执行命令。

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390 | 1）创建 kube-controller-manager 证书和私钥<br>创建证书签名请求：<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cat &gt; kube-controller-manager-csr.json &lt;&lt;EOF<br>{<br>    "CN": "system:kube-controller-manager",<br>    "key": {<br>        "algo": "rsa",<br>        "size": 2048<br>    },<br>    "hosts": [<br>      "127.0.0.1",<br>      "172.16.60.241",<br>      "172.16.60.242",<br>      "172.16.60.243"<br>    ],<br>    "names": [<br>      {<br>        "C": "CN",<br>        "ST": "BeiJing",<br>        "L": "BeiJing",<br>        "O": "system:kube-controller-manager",<br>        "OU": "4Paradigm"<br>      }<br>    ]<br>}<br>EOF<br>  <br>• hosts 列表包含所有 kube-controller-manager 节点 IP；<br>• CN 为 system:kube-controller-manager、O 为 system:kube-controller-manager，kubernetes 内置的 ClusterRoleBindings system:kube-controller-manager<br>  赋予 kube-controller-manager 工作所需的权限。<br>  <br>生成证书和私钥<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cfssl gencert -ca=/opt/k8s/work/ca.pem \\<br>  -ca-key=/opt/k8s/work/ca-key.pem \\<br>  -config=/opt/k8s/work/ca-config.json \\<br>  -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager<br>  <br>[root@k8s-master01 work]\# ll kube-controller-manager\*pem<br>-rw------- 1 root root 1679 Jun 18 11:43 kube-controller-manager-key.pem<br>-rw-r--r-- 1 root root 1517 Jun 18 11:43 kube-controller-manager.pem<br>  <br>将生成的证书和私钥分发到所有 master 节点：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp kube-controller-manager\*.pem root@${node\_master\_ip}:/etc/kubernetes/cert/<br>  done<br>  <br>2) 创建和分发 kubeconfig 文件<br>kube-controller-manager 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-controller-manager 证书：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# kubectl config set-cluster kubernetes \\<br>  --certificate-authority=/opt/k8s/work/ca.pem \\<br>  --embed-certs=true \\<br>  --server=${KUBE\_APISERVER} \\<br>  --kubeconfig=kube-controller-manager.kubeconfig<br>  <br>[root@k8s-master01 work]\# kubectl config set-credentials system:kube-controller-manager \\<br>  --client-certificate=kube-controller-manager.pem \\<br>  --client-key=kube-controller-manager-key.pem \\<br>  --embed-certs=true \\<br>  --kubeconfig=kube-controller-manager.kubeconfig<br>  <br>[root@k8s-master01 work]\# kubectl config set-context system:kube-controller-manager \\<br>  --cluster=kubernetes \\<br>  --user=system:kube-controller-manager \\<br>  --kubeconfig=kube-controller-manager.kubeconfig<br>  <br>[root@k8s-master01 work]\# kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig<br>  <br>分发 kubeconfig 到所有 master 节点：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp kube-controller-manager.kubeconfig root@${node\_master\_ip}:/etc/kubernetes/<br>  done<br>  <br>3) 创建和分发kube-controller-manager system unit 文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# cat &gt; kube-controller-manager.service.template &lt;&lt;EOF<br>[Unit]<br>Description=Kubernetes Controller Manager<br>Documentation=https://github.com/GoogleCloudPlatform/kubernetes<br>  <br>[Service]<br>WorkingDirectory=${K8S\_DIR}/kube-controller-manager<br>ExecStart=/opt/k8s/bin/kube-controller-manager \\\\<br>  --profiling \\\\<br>  --cluster-name=kubernetes \\\\<br>  --controllers=\*,bootstrapsigner,tokencleaner \\\\<br>  --kube-api-qps=1000 \\\\<br>  --kube-api-burst=2000 \\\\<br>  --leader-elect \\\\<br>  --use-service-account-credentials=true \\\\<br>  --concurrent-service-syncs=2 \\\\<br>  --bind-address=0.0.0.0 \\\\<br>  --tls-cert-file=/etc/kubernetes/cert/kube-controller-manager.pem \\\\<br>  --tls-private-key-file=/etc/kubernetes/cert/kube-controller-manager-key.pem \\\\<br>  --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\<br>  --client-ca-file=/etc/kubernetes/cert/ca.pem \\\\<br>  --requestheader-allowed-names="" \\\\<br>  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\\\<br>  --requestheader-extra-headers-prefix="X-Remote-Extra-" \\\\<br>  --requestheader-group-headers=X-Remote-Group \\\\<br>  --requestheader-username-headers=X-Remote-User \\\\<br>  --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\<br>  --cluster-signing-cert-file=/etc/kubernetes/cert/ca.pem \\\\<br>  --cluster-signing-key-file=/etc/kubernetes/cert/ca-key.pem \\\\<br>  --experimental-cluster-signing-duration=8760h \\\\<br>  --horizontal-pod-autoscaler-sync-period=10s \\\\<br>  --concurrent-deployment-syncs=10 \\\\<br>  --concurrent-gc-syncs=30 \\\\<br>  --node-cidr-mask-size=24 \\\\<br>  --service-cluster-ip-range=${SERVICE\_CIDR} \\\\<br>  --pod-eviction-timeout=6m \\\\<br>  --terminated-pod-gc-threshold=10000 \\\\<br>  --root-ca-file=/etc/kubernetes/cert/ca.pem \\\\<br>  --service-account-private-key-file=/etc/kubernetes/cert/ca-key.pem \\\\<br>  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\<br>  --logtostderr=true \\\\<br>  --v=2<br>Restart=on-failure<br>RestartSec=5<br>  <br>[Install]<br>WantedBy=multi-user.target<br>EOF<br>  <br>解释说明：<br>下面两行一般要去掉，否则执行"kubectl get cs"检查集群状态时，controller-manager状态会为"Unhealthy"<br>--port=0：关闭监听非安全端口（http），同时 --address 参数无效，--bind-address 参数有效；<br>--secure-port=10252<br>  <br>--bind-address=0.0.0.0: 在所有网络接口监听 10252 端口的 https /metrics 请求；<br>--kubeconfig：指定 kubeconfig 文件路径，kube-controller-manager 使用它连接和验证 kube-apiserver；<br>--authentication-kubeconfig 和 --authorization-kubeconfig：kube-controller-manager 使用它连接 apiserver，对 client 的请求进行认证和授权。kube-controller-manager 不再使用 --tls-ca-file 对请求 https metrics 的 Client 证书进行校验。如果没有配置这两个 kubeconfig 参数，则 client 连接 kube-controller-manager https 端口的请求会被拒绝(提示权限不足)。<br>--cluster-signing-\*-file：签名 TLS Bootstrap 创建的证书；<br>--experimental-cluster-signing-duration：指定 TLS Bootstrap 证书的有效期；<br>--root-ca-file：放置到容器 ServiceAccount 中的 CA 证书，用来对 kube-apiserver 的证书进行校验；<br>--service-account-private-key-file：签名 ServiceAccount 中 Token 的私钥文件，必须和 kube-apiserver 的 --service-account-key-file 指定的公钥文件配对使用；<br>--service-cluster-ip-range ：指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致；<br>--leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；<br>--controllers=\*,bootstrapsigner,tokencleaner：启用的控制器列表，tokencleaner 用于自动清理过期的 Bootstrap token；<br>--horizontal-pod-autoscaler-\*：custom metrics 相关参数，支持 autoscaling/v2alpha1；<br>--tls-cert-file、--tls-private-key-file：使用 https 输出 metrics 时使用的 Server 证书和秘钥；<br>--use-service-account-credentials=true: kube-controller-manager 中各 controller 使用 serviceaccount 访问 kube-apiserver；<br>  <br>为各节点创建和分发 kube-controller-mananger systemd unit 文件<br>替换模板文件中的变量，为各节点创建 systemd unit 文件：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for (( i=0; i &lt; 3; i++ ))<br>  do<br>    sed -e "s/\#\#NODE\_MASTER\_NAME\#\#/${NODE\_MASTER\_NAMES[i]}/" -e "s/\#\#NODE\_MASTER\_IP\#\#/${NODE\_MASTER\_IPS[i]}/" kube-controller-manager.service.template &gt; kube-controller-manager-${NODE\_MASTER\_IPS[i]}.service<br>  done<br>  <br>注意： NODE\_NAMES 和 NODE\_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；<br>  <br>[root@k8s-master01 work]\# ll kube-controller-manager\*.service<br>-rw-r--r-- 1 root root 1878 Jun 18 12:45 kube-controller-manager-172.16.60.241.service<br>-rw-r--r-- 1 root root 1878 Jun 18 12:45 kube-controller-manager-172.16.60.242.service<br>-rw-r--r-- 1 root root 1878 Jun 18 12:45 kube-controller-manager-172.16.60.243.service<br>  <br>分发到所有 master 节点：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp kube-controller-manager-${node\_master\_ip}.service root@${node\_master\_ip}:/etc/systemd/system/kube-controller-manager.service<br>  done<br>  <br>注意：文件重命名为 kube-controller-manager.service;<br>  <br>启动 kube-controller-manager 服务<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    ssh root@${node\_master\_ip} "mkdir -p ${K8S\_DIR}/kube-controller-manager"<br>    ssh root@${node\_master\_ip} "systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl restart kube-controller-manager"<br>  done<br>  <br>注意：启动服务前必须先创建工作目录；<br>  <br>检查服务运行状态<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    ssh root@${node\_master\_ip} "systemctl status kube-controller-manager|grep Active"<br>  done<br>  <br>预期输出结果：<br>&gt;&gt;&gt; 172.16.60.241<br>   Active: active (running) since Tue 2019-06-18 12:49:11 CST; 1min 7s ago<br>&gt;&gt;&gt; 172.16.60.242<br>   Active: active (running) since Tue 2019-06-18 12:49:11 CST; 1min 7s ago<br>&gt;&gt;&gt; 172.16.60.243<br>   Active: active (running) since Tue 2019-06-18 12:49:12 CST; 1min 7s ago<br>  <br>确保状态为 active (running)，否则查看日志，确认原因（journalctl -u kube-controller-manager）<br>  <br>kube-controller-manager 监听 10252 端口，接收 https 请求：<br>[root@k8s-master01 work]\# netstat -lnpt|grep kube-controll<br>tcp        0      0 172.16.60.241:10252     0.0.0.0:\*               LISTEN      25709/kube-controll<br>  <br>检查集群状态，controller-manager的状态为"ok"<br>注意：当kube-controller-manager集群中的1个或2个节点的controller-manager服务挂掉，只要有一个节点的controller-manager服务活着，<br>则集群中controller-manager的状态仍然为"ok",仍然会继续提供服务！<br>[root@k8s-master01 work]\# kubectl get cs<br>NAME                 STATUS      MESSAGE                                                                                     ERROR<br>scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused <br>controller-manager   Healthy     ok                                                                                        <br>etcd-0               Healthy     {"health":"true"}                                                                         <br>etcd-1               Healthy     {"health":"true"}                                                                         <br>etcd-2               Healthy     {"health":"true"}<br>  <br>4) 查看输出的 metrics<br>注意：以下命令在3台kube-controller-manager节点上执行。<br>  <br>由于在kube-controller-manager启动文件中关掉了"--port=0"和"--secure-port=10252"这两个参数，则只能通过http方式获取到kube-controller-manager<br>输出的metrics信息。kube-controller-manager一般不会被访问，只有在监控时采集metrcis指标数据时被访问。<br>  <br>[root@k8s-master01 work]\# curl -s http://172.16.60.241:10252/metrics|head                                    <br>\# HELP apiserver\_audit\_event\_total Counter of audit events generated and sent to the audit backend.<br>\# TYPE apiserver\_audit\_event\_total counter<br>apiserver\_audit\_event\_total 0<br>\# HELP apiserver\_audit\_requests\_rejected\_total Counter of apiserver requests rejected due to an error in audit logging backend.<br>\# TYPE apiserver\_audit\_requests\_rejected\_total counter<br>apiserver\_audit\_requests\_rejected\_total 0<br>\# HELP apiserver\_client\_certificate\_expiration\_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.<br>\# TYPE apiserver\_client\_certificate\_expiration\_seconds histogram<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="0"} 0<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="1800"} 0<br>  <br>[root@k8s-master01 work]\# curl -s --cacert /etc/kubernetes/cert/ca.pem http://172.16.60.241:10252/metrics |head<br>\# HELP apiserver\_audit\_event\_total Counter of audit events generated and sent to the audit backend.<br>\# TYPE apiserver\_audit\_event\_total counter<br>apiserver\_audit\_event\_total 0<br>\# HELP apiserver\_audit\_requests\_rejected\_total Counter of apiserver requests rejected due to an error in audit logging backend.<br>\# TYPE apiserver\_audit\_requests\_rejected\_total counter<br>apiserver\_audit\_requests\_rejected\_total 0<br>\# HELP apiserver\_client\_certificate\_expiration\_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.<br>\# TYPE apiserver\_client\_certificate\_expiration\_seconds histogram<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="0"} 0<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="1800"} 0<br>  <br>[root@k8s-master01 work]\# curl -s --cacert /etc/kubernetes/cert/ca.pem http://127.0.0.1:10252/metrics |head  <br>\# HELP apiserver\_audit\_event\_total Counter of audit events generated and sent to the audit backend.<br>\# TYPE apiserver\_audit\_event\_total counter<br>apiserver\_audit\_event\_total 0<br>\# HELP apiserver\_audit\_requests\_rejected\_total Counter of apiserver requests rejected due to an error in audit logging backend.<br>\# TYPE apiserver\_audit\_requests\_rejected\_total counter<br>apiserver\_audit\_requests\_rejected\_total 0<br>\# HELP apiserver\_client\_certificate\_expiration\_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.<br>\# TYPE apiserver\_client\_certificate\_expiration\_seconds histogram<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="0"} 0<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="1800"} 0<br> <br>[root@k8s-master01 ~]\# curl -s --cacert /opt/k8s/work/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem http://172.16.60.241:10252/metrics |head<br>\# HELP apiserver\_audit\_event\_total Counter of audit events generated and sent to the audit backend.<br>\# TYPE apiserver\_audit\_event\_total counter<br>apiserver\_audit\_event\_total 0<br>\# HELP apiserver\_audit\_requests\_rejected\_total Counter of apiserver requests rejected due to an error in audit logging backend.<br>\# TYPE apiserver\_audit\_requests\_rejected\_total counter<br>apiserver\_audit\_requests\_rejected\_total 0<br>\# HELP apiserver\_client\_certificate\_expiration\_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.<br>\# TYPE apiserver\_client\_certificate\_expiration\_seconds histogram<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="0"} 0<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="1800"} 0<br>  <br>5) kube-controller-manager 的权限<br>ClusteRole system:kube-controller-manager 的权限很小，只能创建 secret、serviceaccount 等资源对象，各 controller 的权限分散到 ClusterRole system:controller:XXX 中：<br>  <br>[root@k8s-master01 work]\# kubectl describe clusterrole system:kube-controller-manager<br>Name:         system:kube-controller-manager<br>Labels:       kubernetes.io/bootstrapping=rbac-defaults<br>Annotations:  rbac.authorization.kubernetes.io/autoupdate: true<br>PolicyRule:<br>  Resources                                  Non-Resource URLs  Resource Names  Verbs<br>  ---------                                  -----------------  --------------  -----<br>  secrets                                    []                 []              [create delete get update]<br>  endpoints                                  []                 []              [create get update]<br>  serviceaccounts                            []                 []              [create get update]<br>  events                                     []                 []              [create patch update]<br>  tokenreviews.authentication.k8s.io         []                 []              [create]<br>  subjectaccessreviews.authorization.k8s.io  []                 []              [create]<br>  configmaps                                 []                 []              [get]<br>  namespaces                                 []                 []              [get]<br>  \*.\*                                        []                 []              [list watch]<br>  <br>需要在 kube-controller-manager 的启动参数中添加 --use-service-account-credentials=true 参数，这样 main controller 会为各 controller 创建对应的 ServiceAccount XXX-controller。<br>内置的 ClusterRoleBinding system:controller:XXX 将赋予各 XXX-controller ServiceAccount 对应的 ClusterRole system:controller:XXX 权限。<br>  <br>[root@k8s-master01 work]\# kubectl get clusterrole|grep controller<br>system:controller:attachdetach-controller                              141m<br>system:controller:certificate-controller                               141m<br>system:controller:clusterrole-aggregation-controller                   141m<br>system:controller:cronjob-controller                                   141m<br>system:controller:daemon-set-controller                                141m<br>system:controller:deployment-controller                                141m<br>system:controller:disruption-controller                                141m<br>system:controller:endpoint-controller                                  141m<br>system:controller:expand-controller                                    141m<br>system:controller:generic-garbage-collector                            141m<br>system:controller:horizontal-pod-autoscaler                            141m<br>system:controller:job-controller                                       141m<br>system:controller:namespace-controller                                 141m<br>system:controller:node-controller                                      141m<br>system:controller:persistent-volume-binder                             141m<br>system:controller:pod-garbage-collector                                141m<br>system:controller:pv-protection-controller                             141m<br>system:controller:pvc-protection-controller                            141m<br>system:controller:replicaset-controller                                141m<br>system:controller:replication-controller                               141m<br>system:controller:resourcequota-controller                             141m<br>system:controller:route-controller                                     141m<br>system:controller:service-account-controller                           141m<br>system:controller:service-controller                                   141m<br>system:controller:statefulset-controller                               141m<br>system:controller:ttl-controller                                       141m<br>system:kube-controller-manager                                         141m<br>  <br>以 deployment controller 为例：<br>[root@k8s-master01 work]\# kubectl describe clusterrole system:controller:deployment-controller<br>Name:         system:controller:deployment-controller<br>Labels:       kubernetes.io/bootstrapping=rbac-defaults<br>Annotations:  rbac.authorization.kubernetes.io/autoupdate: true<br>PolicyRule:<br>  Resources                          Non-Resource URLs  Resource Names  Verbs<br>  ---------                          -----------------  --------------  -----<br>  replicasets.apps                   []                 []              [create delete get list patch update watch]<br>  replicasets.extensions             []                 []              [create delete get list patch update watch]<br>  events                             []                 []              [create patch update]<br>  pods                               []                 []              [get list update watch]<br>  deployments.apps                   []                 []              [get list update watch]<br>  deployments.extensions             []                 []              [get list update watch]<br>  deployments.apps/finalizers        []                 []              [update]<br>  deployments.apps/status            []                 []              [update]<br>  deployments.extensions/finalizers  []                 []              [update]<br>  deployments.extensions/status      []                 []              [update]<br>  <br>6）查看kube-controller-manager集群中当前的leader<br>[root@k8s-master01 work]\# kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml<br>apiVersion: v1<br>kind: Endpoints<br>metadata:<br>  annotations:<br>    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"k8s-master02\_4e449819-9185-11e9-82b6-005056ac42a4","leaseDurationSeconds":15,"acquireTime":"2019-06-18T04:55:49Z","renewTime":"2019-06-18T05:04:54Z","leaderTransitions":3}'<br>  creationTimestamp: "2019-06-18T04:03:07Z"<br>  name: kube-controller-manager<br>  namespace: kube-system<br>  resourceVersion: "4604"<br>  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager<br>  uid: fa824018-917d-11e9-90d4-005056ac7c81<br>  <br>可见，当前的leader为k8s-master02节点。<br>  <br>测试 kube-controller-manager 集群的高可用<br>停掉一个或两个节点的 kube-controller-manager 服务，观察其它节点的日志，看是否获取了 leader 权限。<br>  <br>比如停掉k8s-master02节点的kube-controller-manager 服务<br>[root@k8s-master02 ~]\# systemctl stop kube-controller-manager <br>[root@k8s-master02 ~]\# ps -ef|grep kube-controller-manager<br>root     25677 11006  0 13:06 pts/0    00:00:00 grep --color=auto kube-controller-manager<br>  <br>接着观察kube-controller-manager集群当前的leader情况<br>[root@k8s-master01 work]\# kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml<br>apiVersion: v1<br>kind: Endpoints<br>metadata:<br>  annotations:<br>    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"k8s-master03\_4e4c28b5-9185-11e9-b98a-005056ac7136","leaseDurationSeconds":15,"acquireTime":"2019-06-18T05:06:32Z","renewTime":"2019-06-18T05:06:57Z","leaderTransitions":4}'<br>  creationTimestamp: "2019-06-18T04:03:07Z"<br>  name: kube-controller-manager<br>  namespace: kube-system<br>  resourceVersion: "4695"<br>  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager<br>  uid: fa824018-917d-11e9-90d4-005056ac7c81<br>  <br>发现当前leader已经转移到k8s-master03节点上了！！ |


8.3 - 部署高可用 kube-scheduler 集群

该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。为保证通信安全，本文档先生成 x509 证书和私钥，kube-scheduler 在如下两种情况下使用该证书：与kube-apiserver 的安全端口通信;在安全端口(https，10251) 输出 prometheus 格式的 metrics；下面部署命令均在k8s-master01节点上执行，然后远程分发文件和执行命令。

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345 | 1）创建 kube-scheduler 证书和私钥<br>创建证书签名请求：<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cat &gt; kube-scheduler-csr.json &lt;&lt;EOF<br>{<br>    "CN": "system:kube-scheduler",<br>    "hosts": [<br>      "127.0.0.1",<br>      "172.16.60.241",<br>      "172.16.60.242",<br>      "172.16.60.243"<br>    ],<br>    "key": {<br>        "algo": "rsa",<br>        "size": 2048<br>    },<br>    "names": [<br>      {<br>        "C": "CN",<br>        "ST": "BeiJing",<br>        "L": "BeiJing",<br>        "O": "system:kube-scheduler",<br>        "OU": "4Paradigm"<br>      }<br>    ]<br>}<br>EOF<br> <br>解释说明：<br>hosts 列表包含所有 kube-scheduler 节点 IP；<br>CN 和 O 均为 system:kube-scheduler，kubernetes 内置的 ClusterRoleBindings system:kube-scheduler 将赋予 kube-scheduler 工作所需的权限；<br> <br>生成证书和私钥：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cfssl gencert -ca=/opt/k8s/work/ca.pem \\<br>  -ca-key=/opt/k8s/work/ca-key.pem \\<br>  -config=/opt/k8s/work/ca-config.json \\<br>  -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler<br> <br>[root@k8s-master01 work]\# ls kube-scheduler\*pem<br>kube-scheduler-key.pem  kube-scheduler.pem<br> <br>将生成的证书和私钥分发到所有 master 节点：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp kube-scheduler\*.pem root@${node\_master\_ip}:/etc/kubernetes/cert/<br>  done<br> <br>2) 创建和分发 kubeconfig 文件<br>kube-scheduler 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-scheduler 证书：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# kubectl config set-cluster kubernetes \\<br>  --certificate-authority=/opt/k8s/work/ca.pem \\<br>  --embed-certs=true \\<br>  --server=${KUBE\_APISERVER} \\<br>  --kubeconfig=kube-scheduler.kubeconfig<br> <br>[root@k8s-master01 work]\# kubectl config set-credentials system:kube-scheduler \\<br>  --client-certificate=kube-scheduler.pem \\<br>  --client-key=kube-scheduler-key.pem \\<br>  --embed-certs=true \\<br>  --kubeconfig=kube-scheduler.kubeconfig<br> <br>[root@k8s-master01 work]\# kubectl config set-context system:kube-scheduler \\<br>  --cluster=kubernetes \\<br>  --user=system:kube-scheduler \\<br>  --kubeconfig=kube-scheduler.kubeconfig<br> <br>[root@k8s-master01 work]\# kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig<br> <br>分发 kubeconfig 到所有 master 节点：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp kube-scheduler.kubeconfig root@${node\_master\_ip}:/etc/kubernetes/<br>  done<br> <br>3) 创建 kube-scheduler 配置文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cat &gt;kube-scheduler.yaml.template &lt;&lt;EOF<br>apiVersion: kubescheduler.config.k8s.io/v1alpha1<br>kind: KubeSchedulerConfiguration<br>bindTimeoutSeconds: 600<br>clientConnection:<br>  burst: 200<br>  kubeconfig: "/etc/kubernetes/kube-scheduler.kubeconfig"<br>  qps: 100<br>enableContentionProfiling: false<br>enableProfiling: true<br>hardPodAffinitySymmetricWeight: 1<br>healthzBindAddress: 0.0.0.0:10251<br>leaderElection:<br>  leaderElect: true<br>metricsBindAddress: 0.0.0.0:10251<br>EOF<br> <br>注意：这里的ip地址最好用0.0.0.0，不然执行"kubectl get cs"查看schedule的集群状态会是"Unhealthy"<br>--kubeconfig：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；<br>--leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；<br> <br>替换模板文件中的变量：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for (( i=0; i &lt; 3; i++ ))<br>  do<br>    sed -e "s/\#\#NODE\_MASTER\_NAME\#\#/${NODE\_MASTER\_NAMES[i]}/" -e "s/\#\#NODE\_MASTER\_IP\#\#/${NODE\_MASTER\_IPS[i]}/" kube-scheduler.yaml.template &gt; kube-scheduler-${NODE\_MASTER\_IPS[i]}.yaml<br>  done<br> <br>注意：NODE\_NAMES 和 NODE\_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；<br> <br>[root@k8s-master01 work]\# ll kube-scheduler\*.yaml<br>-rw-r--r-- 1 root root 399 Jun 18 14:57 kube-scheduler-172.16.60.241.yaml<br>-rw-r--r-- 1 root root 399 Jun 18 14:57 kube-scheduler-172.16.60.242.yaml<br>-rw-r--r-- 1 root root 399 Jun 18 14:57 kube-scheduler-172.16.60.243.yaml<br> <br>分发 kube-scheduler 配置文件到所有 master 节点：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp kube-scheduler-${node\_master\_ip}.yaml root@${node\_master\_ip}:/etc/kubernetes/kube-scheduler.yaml<br>  done<br> <br>注意：重命名为 kube-scheduler.yaml;<br> <br>4）创建 kube-scheduler systemd unit 模板文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cat &gt; kube-scheduler.service.template &lt;&lt;EOF<br>[Unit]<br>Description=Kubernetes Scheduler<br>Documentation=https://github.com/GoogleCloudPlatform/kubernetes<br> <br>[Service]<br>WorkingDirectory=${K8S\_DIR}/kube-scheduler<br>ExecStart=/opt/k8s/bin/kube-scheduler \\\\<br>  --config=/etc/kubernetes/kube-scheduler.yaml \\\\<br>  --bind-address=0.0.0.0 \\\\<br>  --tls-cert-file=/etc/kubernetes/cert/kube-scheduler.pem \\\\<br>  --tls-private-key-file=/etc/kubernetes/cert/kube-scheduler-key.pem \\\\<br>  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\\\<br>  --client-ca-file=/etc/kubernetes/cert/ca.pem \\\\<br>  --requestheader-allowed-names="" \\\\<br>  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\\\<br>  --requestheader-extra-headers-prefix="X-Remote-Extra-" \\\\<br>  --requestheader-group-headers=X-Remote-Group \\\\<br>  --requestheader-username-headers=X-Remote-User \\\\<br>  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\\\<br>  --logtostderr=true \\\\<br>  --v=2<br>Restart=always<br>RestartSec=5<br>StartLimitInterval=0<br> <br>[Install]<br>WantedBy=multi-user.target<br>EOF<br> <br>为各节点创建和分发 kube-scheduler systemd unit 文件<br>替换模板文件中的变量，为各节点创建 systemd unit 文件：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for (( i=0; i &lt; 3; i++ ))<br>  do<br>    sed -e "s/\#\#NODE\_MASTER\_NAME\#\#/${NODE\_MASTER\_NAMES[i]}/" -e "s/\#\#NODE\_MASTER\_IP\#\#/${NODE\_MASTER\_IPS[i]}/" kube-scheduler.service.template &gt; kube-scheduler-${NODE\_MASTER\_IPS[i]}.service<br>  done<br> <br>其中：NODE\_NAMES 和 NODE\_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；<br> <br>[root@k8s-master01 work]\# ll kube-scheduler\*.service<br>-rw-r--r-- 1 root root 981 Jun 18 15:30 kube-scheduler-172.16.60.241.service<br>-rw-r--r-- 1 root root 981 Jun 18 15:30 kube-scheduler-172.16.60.242.service<br>-rw-r--r-- 1 root root 981 Jun 18 15:30 kube-scheduler-172.16.60.243.service<br> <br>分发 systemd unit 文件到所有 master 节点：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    scp kube-scheduler-${node\_master\_ip}.service root@${node\_master\_ip}:/etc/systemd/system/kube-scheduler.service<br>  done<br> <br>5) 启动 kube-scheduler 服务<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    ssh root@${node\_master\_ip} "mkdir -p ${K8S\_DIR}/kube-scheduler"<br>    ssh root@${node\_master\_ip} "systemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl restart kube-scheduler"<br>  done<br> <br>注意：启动服务前必须先创建工作目录；<br> <br>检查服务运行状态<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_master\_ip in ${NODE\_MASTER\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_master\_ip}"<br>    ssh root@${node\_master\_ip} "systemctl status kube-scheduler|grep Active"<br>  done<br> <br>预期输出结果:<br>&gt;&gt;&gt; 172.16.60.241<br>   Active: active (running) since Tue 2019-06-18 15:33:29 CST; 1min 12s ago<br>&gt;&gt;&gt; 172.16.60.242<br>   Active: active (running) since Tue 2019-06-18 15:33:30 CST; 1min 11s ago<br>&gt;&gt;&gt; 172.16.60.243<br>   Active: active (running) since Tue 2019-06-18 15:33:30 CST; 1min 11s ago<br> <br>确保状态为 active (running)，否则查看日志，确认原因: (journalctl -u kube-scheduler)<br> <br>看看集群状态，此时状态均为"ok"<br>[root@k8s-master01 work]\# kubectl get cs<br>NAME                 STATUS    MESSAGE             ERROR<br>scheduler            Healthy   ok                 <br>controller-manager   Healthy   ok                 <br>etcd-2               Healthy   {"health":"true"}  <br>etcd-0               Healthy   {"health":"true"}  <br>etcd-1               Healthy   {"health":"true"}<br> <br>6) 查看输出的 metrics<br>注意：以下命令要在kube-scheduler集群节点上执行。<br> <br>kube-scheduler监听10251和10259端口：<br>10251：接收 http 请求，非安全端口，不需要认证授权；<br>10259：接收 https 请求，安全端口，需要认证授权；<br>两个接口都对外提供 /metrics 和 /healthz 的访问。<br> <br>[root@k8s-master01 work]\# netstat -lnpt |grep kube-schedule<br>tcp6       0      0 :::10251                :::\*                    LISTEN      6075/kube-scheduler<br>tcp6       0      0 :::10259                :::\*                    LISTEN      6075/kube-scheduler<br> <br>[root@k8s-master01 work]\# lsof -i:10251                   <br>COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME<br>kube-sche 6075 root    3u  IPv6 628571      0t0  TCP \*:10251 (LISTEN)<br> <br>[root@k8s-master01 work]\# lsof -i:10259                   <br>COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME<br>kube-sche 6075 root    5u  IPv6 628574      0t0  TCP \*:10259 (LISTEN)<br> <br>下面几种方式均能获取到kube-schedule的metrics数据信息（分别使用http的10251 和 https的10259端口）<br>[root@k8s-master01 work]\# curl -s http://172.16.60.241:10251/metrics |head       <br>\# HELP apiserver\_audit\_event\_total Counter of audit events generated and sent to the audit backend.<br>\# TYPE apiserver\_audit\_event\_total counter<br>apiserver\_audit\_event\_total 0<br>\# HELP apiserver\_audit\_requests\_rejected\_total Counter of apiserver requests rejected due to an error in audit logging backend.<br>\# TYPE apiserver\_audit\_requests\_rejected\_total counter<br>apiserver\_audit\_requests\_rejected\_total 0<br>\# HELP apiserver\_client\_certificate\_expiration\_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.<br>\# TYPE apiserver\_client\_certificate\_expiration\_seconds histogram<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="0"} 0<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="1800"} 0<br> <br>[root@k8s-master01 work]\# curl -s http://127.0.0.1:10251/metrics |head   <br>\# HELP apiserver\_audit\_event\_total Counter of audit events generated and sent to the audit backend.<br>\# TYPE apiserver\_audit\_event\_total counter<br>apiserver\_audit\_event\_total 0<br>\# HELP apiserver\_audit\_requests\_rejected\_total Counter of apiserver requests rejected due to an error in audit logging backend.<br>\# TYPE apiserver\_audit\_requests\_rejected\_total counter<br>apiserver\_audit\_requests\_rejected\_total 0<br>\# HELP apiserver\_client\_certificate\_expiration\_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.<br>\# TYPE apiserver\_client\_certificate\_expiration\_seconds histogram<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="0"} 0<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="1800"} 0<br> <br>[root@k8s-master01 work]\# curl -s --cacert /etc/kubernetes/cert/ca.pem http://172.16.60.241:10251/metrics |head  <br>\# HELP apiserver\_audit\_event\_total Counter of audit events generated and sent to the audit backend.<br>\# TYPE apiserver\_audit\_event\_total counter<br>apiserver\_audit\_event\_total 0<br>\# HELP apiserver\_audit\_requests\_rejected\_total Counter of apiserver requests rejected due to an error in audit logging backend.<br>\# TYPE apiserver\_audit\_requests\_rejected\_total counter<br>apiserver\_audit\_requests\_rejected\_total 0<br>\# HELP apiserver\_client\_certificate\_expiration\_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.<br>\# TYPE apiserver\_client\_certificate\_expiration\_seconds histogram<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="0"} 0<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="1800"} 0<br> <br>[root@k8s-master01 work]\# curl -s --cacert /etc/kubernetes/cert/ca.pem http://127.0.0.1:10251/metrics |head      <br>\# HELP apiserver\_audit\_event\_total Counter of audit events generated and sent to the audit backend.<br>\# TYPE apiserver\_audit\_event\_total counter<br>apiserver\_audit\_event\_total 0<br>\# HELP apiserver\_audit\_requests\_rejected\_total Counter of apiserver requests rejected due to an error in audit logging backend.<br>\# TYPE apiserver\_audit\_requests\_rejected\_total counter<br>apiserver\_audit\_requests\_rejected\_total 0<br>\# HELP apiserver\_client\_certificate\_expiration\_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.<br>\# TYPE apiserver\_client\_certificate\_expiration\_seconds histogram<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="0"} 0<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="1800"} 0<br> <br>[root@k8s-master01 work]\# curl -s --cacert /opt/k8s/work/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://172.16.60.241:10259/metrics |head<br>\# HELP apiserver\_audit\_event\_total Counter of audit events generated and sent to the audit backend.<br>\# TYPE apiserver\_audit\_event\_total counter<br>apiserver\_audit\_event\_total 0<br>\# HELP apiserver\_audit\_requests\_rejected\_total Counter of apiserver requests rejected due to an error in audit logging backend.<br>\# TYPE apiserver\_audit\_requests\_rejected\_total counter<br>apiserver\_audit\_requests\_rejected\_total 0<br>\# HELP apiserver\_client\_certificate\_expiration\_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.<br>\# TYPE apiserver\_client\_certificate\_expiration\_seconds histogram<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="0"} 0<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="1800"} 0<br> <br>7）查看当前的 leader<br>[root@k8s-master01 work]\# kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml<br>apiVersion: v1<br>kind: Endpoints<br>metadata:<br>  annotations:<br>    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"k8s-master01\_5eac29d7-919b-11e9-b242-005056ac7c81","leaseDurationSeconds":15,"acquireTime":"2019-06-18T07:33:31Z","renewTime":"2019-06-18T07:41:13Z","leaderTransitions":0}'<br>  creationTimestamp: "2019-06-18T07:33:31Z"<br>  name: kube-scheduler<br>  namespace: kube-system<br>  resourceVersion: "12218"<br>  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler<br>  uid: 5f466875-919b-11e9-90d4-005056ac7c81<br> <br>可见，当前的 leader 为 k8s-master01 节点。<br> <br>测试 kube-scheduler 集群的高可用<br>随便找一个或两个 master 节点，停掉 kube-scheduler 服务，看其它节点是否获取了 leader 权限。<br> <br>比如停掉k8s-master01节点的kube-schedule服务，查看下leader的转移情况<br>[root@k8s-master01 work]\# systemctl stop kube-scheduler<br>[root@k8s-master01 work]\# ps -ef|grep kube-scheduler<br>root      6871  2379  0 15:42 pts/2    00:00:00 grep --color=auto kube-scheduler<br> <br>再次看看当前的leader，发现leader已经转移为k8s-master02节点了<br>[root@k8s-master01 work]\# kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml<br>apiVersion: v1<br>kind: Endpoints<br>metadata:<br>  annotations:<br>    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"k8s-master02\_5efade79-919b-11e9-bbe2-005056ac42a4","leaseDurationSeconds":15,"acquireTime":"2019-06-18T07:43:03Z","renewTime":"2019-06-18T07:43:12Z","leaderTransitions":1}'<br>  creationTimestamp: "2019-06-18T07:33:31Z"<br>  name: kube-scheduler<br>  namespace: kube-system<br>  resourceVersion: "12363"<br>  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler<br>  uid: 5f466875-919b-11e9-90d4-005056ac7c81 |


九、部署node工作节点

kubernetes node节点运行的组件有docker、kubelet、kube-proxy、flanneld。下面部署命令均在k8s-master01节点上执行，然后远程分发文件和执行命令。

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8 | 安装依赖包<br>[root@k8s-master01 ~]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 ~]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "yum install -y epel-release"<br>    ssh root@${node\_node\_ip} "yum install -y conntrack ipvsadm ntp ntpdate ipset jq iptables curl sysstat libseccomp &amp;&amp; modprobe ip\_vs "<br>  done |


9.1 - 部署 docker 组件

docker 运行和管理容器，kubelet 通过 Container Runtime Interface (CRI) 与它进行交互。下面操作均在k8s-master01上执行，然后远程分发文件和执行命令。

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216 | 1) 下载和分发 docker 二进制文件<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# wget https://download.docker.com/linux/static/stable/x86\_64/docker-18.09.6.tgz<br>[root@k8s-master01 work]\# tar -xvf docker-18.09.6.tgz<br> <br>分发二进制文件到所有node节点：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    scp docker/\*  root@${node\_node\_ip}:/opt/k8s/bin/<br>    ssh root@${node\_node\_ip} "chmod +x /opt/k8s/bin/\*"<br>  done<br> <br>2) 创建和分发 systemd unit 文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cat &gt; docker.service &lt;&lt;"EOF"<br>[Unit]<br>Description=Docker Application Container Engine<br>Documentation=http://docs.docker.io<br> <br>[Service]<br>WorkingDirectory=\#\#DOCKER\_DIR\#\#<br>Environment="PATH=/opt/k8s/bin:/bin:/sbin:/usr/bin:/usr/sbin"<br>EnvironmentFile=-/run/flannel/docker<br>ExecStart=/opt/k8s/bin/dockerd $DOCKER\_NETWORK\_OPTIONS<br>ExecReload=/bin/kill -s HUP $MAINPID<br>Restart=on-failure<br>RestartSec=5<br>LimitNOFILE=infinity<br>LimitNPROC=infinity<br>LimitCORE=infinity<br>Delegate=yes<br>KillMode=process<br> <br>[Install]<br>WantedBy=multi-user.target<br>EOF<br> <br>注意事项：<br>-&gt; EOF 前后有双引号，这样 bash 不会替换文档中的变量，如 $DOCKER\_NETWORK\_OPTIONS (这些环境变量是 systemd 负责替换的。)；<br>-&gt; dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中；<br>-&gt; flanneld 启动时将网络配置写入 /run/flannel/docker 文件中，dockerd 启动前读取该文件中的环境变量 DOCKER\_NETWORK\_OPTIONS ，然后设置 docker0 网桥网段；<br>-&gt; 如果指定了多个 EnvironmentFile 选项，则必须将 /run/flannel/docker 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；<br>-&gt; docker 需要以 root 用于运行；<br>-&gt; docker 从 1.13 版本开始，可能将 iptables FORWARD chain的默认策略设置为DROP，从而导致 ping 其它 Node 上的 Pod IP 失败，遇到这种情况时，需要手动设置策略为 ACCEPT：<br>   \# iptables -P FORWARD ACCEPT<br>   并且把以下命令写入 /etc/rc.local 文件中，防止节点重启iptables FORWARD chain的默认策略又还原为DROP<br>   \# /sbin/iptables -P FORWARD ACCEPT<br> <br>分发 systemd unit 文件到所有node节点机器:<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# sed -i -e "s|\#\#DOCKER\_DIR\#\#|${DOCKER\_DIR}|" docker.service<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    scp docker.service root@${node\_node\_ip}:/etc/systemd/system/<br>  done<br> <br>3) 配置和分发 docker 配置文件<br>使用国内的仓库镜像服务器以加快 pull image 的速度，同时增加下载的并发数 (需要重启 dockerd 生效)：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# cat &gt; docker-daemon.json &lt;&lt;EOF<br>{<br>    "registry-mirrors": ["https://docker.mirrors.ustc.edu.cn","https://hub-mirror.c.163.com"],<br>    "insecure-registries": ["docker02:35000"],<br>    "max-concurrent-downloads": 20,<br>    "live-restore": true,<br>    "max-concurrent-uploads": 10,<br>    "debug": true,<br>    "data-root": "${DOCKER\_DIR}/data",<br>    "exec-root": "${DOCKER\_DIR}/exec",<br>    "log-opts": {<br>      "max-size": "100m",<br>      "max-file": "5"<br>    }<br>}<br>EOF<br> <br>分发 docker 配置文件到所有 node 节点：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "mkdir -p  /etc/docker/ ${DOCKER\_DIR}/{data,exec}"<br>    scp docker-daemon.json root@${node\_node\_ip}:/etc/docker/daemon.json<br>  done<br> <br>4) 启动 docker 服务<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker"<br>  done<br> <br>检查服务运行状态<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "systemctl status docker|grep Active"<br>  done<br> <br>预期输出结果：<br>&gt;&gt;&gt; 172.16.60.244<br>   Active: active (running) since Tue 2019-06-18 16:28:32 CST; 42s ago<br>&gt;&gt;&gt; 172.16.60.245<br>   Active: active (running) since Tue 2019-06-18 16:28:31 CST; 42s ago<br>&gt;&gt;&gt; 172.16.60.246<br>   Active: active (running) since Tue 2019-06-18 16:28:32 CST; 42s ago<br> <br>确保状态为 active (running)，否则查看日志，确认原因 (journalctl -u docker)<br> <br>5) 检查 docker0 网桥<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "/usr/sbin/ip addr show flannel.1 &amp;&amp; /usr/sbin/ip addr show docker0"<br>  done<br> <br>预期输出结果：<br>&gt;&gt;&gt; 172.16.60.244<br>3: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER\_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default<br>    link/ether c6:c2:d1:5a:9a:8a brd ff:ff:ff:ff:ff:ff<br>    inet 172.30.88.0/32 scope global flannel.1<br>       valid\_lft forever preferred\_lft forever<br>4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default<br>    link/ether 02:42:27:3c:5e:5f brd ff:ff:ff:ff:ff:ff<br>    inet 172.30.88.1/21 brd 172.30.95.255 scope global docker0<br>       valid\_lft forever preferred\_lft forever<br>&gt;&gt;&gt; 172.16.60.245<br>3: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER\_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default<br>    link/ether 02:36:1d:ab:c4:86 brd ff:ff:ff:ff:ff:ff<br>    inet 172.30.56.0/32 scope global flannel.1<br>       valid\_lft forever preferred\_lft forever<br>4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default<br>    link/ether 02:42:6f:36:7d:fb brd ff:ff:ff:ff:ff:ff<br>    inet 172.30.56.1/21 brd 172.30.63.255 scope global docker0<br>       valid\_lft forever preferred\_lft forever<br>&gt;&gt;&gt; 172.16.60.246<br>3: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER\_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default<br>    link/ether 4e:73:d1:0e:27:c0 brd ff:ff:ff:ff:ff:ff<br>    inet 172.30.72.0/32 scope global flannel.1<br>       valid\_lft forever preferred\_lft forever<br>4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default<br>    link/ether 02:42:21:39:f4:9e brd ff:ff:ff:ff:ff:ff<br>    inet 172.30.72.1/21 brd 172.30.79.255 scope global docker0<br>       valid\_lft forever preferred\_lft forever<br> <br>确认各node节点的docker0网桥和flannel.1接口的IP一定要处于同一个网段中(如下 172.30.88.0/32 位于 172.30.88.1/21 中)！！！<br> <br>到任意一个node节点上查看 docker 的状态信息<br>[root@k8s-node01 ~]\# ps -elfH|grep docker<br>0 S root     21573 18744  0  80   0 - 28180 pipe\_w 16:32 pts/2    00:00:00         grep --color=auto docker<br>4 S root     21147     1  0  80   0 - 173769 futex\_ 16:28 ?       00:00:00   /opt/k8s/bin/dockerd --bip=172.30.88.1/21 --ip-masq=false --mtu=1450<br>4 S root     21175 21147  0  80   0 - 120415 futex\_ 16:28 ?       00:00:00     containerd --config /data/k8s/docker/exec/containerd/containerd.toml --log-level debug<br> <br>[root@k8s-node01 ~]\# docker info<br>Containers: 0<br> Running: 0<br> Paused: 0<br> Stopped: 0<br>Images: 0<br>Server Version: 18.09.6<br>Storage Driver: overlay2<br> Backing Filesystem: xfs<br> Supports d\_type: true<br> Native Overlay Diff: true<br>Logging Driver: json-file<br>Cgroup Driver: cgroupfs<br>Plugins:<br> Volume: local<br> Network: bridge host macvlan null overlay<br> Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog<br>Swarm: inactive<br>Runtimes: runc<br>Default Runtime: runc<br>Init Binary: docker-init<br>containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84<br>runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30<br>init version: fec3683<br>Security Options:<br> seccomp<br>  Profile: default<br>Kernel Version: 4.4.181-1.el7.elrepo.x86\_64<br>Operating System: CentOS Linux 7 (Core)<br>OSType: linux<br>Architecture: x86\_64<br>CPUs: 4<br>Total Memory: 3.859GiB<br>Name: k8s-node01<br>ID: R24D:75E5:2OWS:SNU5:NPSE:SBKH:WKLZ:2ZH7:6ITY:3BE2:YHRG:6WRU<br>Docker Root Dir: /data/k8s/docker/data<br>Debug Mode (client): false<br>Debug Mode (server): true<br> File Descriptors: 22<br> Goroutines: 43<br> System Time: 2019-06-18T16:32:44.260301822+08:00<br> EventsListeners: 0<br>Registry: https://index.docker.io/v1/<br>Labels:<br>Experimental: false<br>Insecure Registries:<br> docker02:35000<br> 127.0.0.0/8<br>Registry Mirrors:<br> https://docker.mirrors.ustc.edu.cn/<br> https://hub-mirror.c.163.com/<br>Live Restore Enabled: true<br>Product License: Community Engine |


9.2 - 部署 kubelet 组件

kubelet 运行在每个node节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如 exec、run、logs 等。kubelet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况。为确保安全，部署时关闭了 kubelet 的非安全 http 端口，对请求进行认证和授权，拒绝未授权的访问(如 apiserver、heapster 的请求)。下面部署命令均在k8s-master01节点上执行，然后远程分发文件和执行命令。

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632 | 1）下载和分发 kubelet 二进制文件<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    scp kubernetes/server/bin/kubelet root@${node\_node\_ip}:/opt/k8s/bin/<br>    ssh root@${node\_node\_ip} "chmod +x /opt/k8s/bin/\*"<br>  done<br>    <br>2）创建 kubelet bootstrap kubeconfig 文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_name in ${NODE\_NODE\_NAMES[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_name}"<br>    <br>    \# 创建 token<br>    export BOOTSTRAP\_TOKEN=$(kubeadm token create \\<br>      --description kubelet-bootstrap-token \\<br>      --groups system:bootstrappers:${node\_node\_name} \\<br>      --kubeconfig ~/.kube/config)<br>    <br>    \# 设置集群参数<br>    kubectl config set-cluster kubernetes \\<br>      --certificate-authority=/etc/kubernetes/cert/ca.pem \\<br>      --embed-certs=true \\<br>      --server=${KUBE\_APISERVER} \\<br>      --kubeconfig=kubelet-bootstrap-${node\_node\_name}.kubeconfig<br>    <br>    \# 设置客户端认证参数<br>    kubectl config set-credentials kubelet-bootstrap \\<br>      --token=${BOOTSTRAP\_TOKEN} \\<br>      --kubeconfig=kubelet-bootstrap-${node\_node\_name}.kubeconfig<br>    <br>    \# 设置上下文参数<br>    kubectl config set-context default \\<br>      --cluster=kubernetes \\<br>      --user=kubelet-bootstrap \\<br>      --kubeconfig=kubelet-bootstrap-${node\_node\_name}.kubeconfig<br>    <br>    \# 设置默认上下文<br>    kubectl config use-context default --kubeconfig=kubelet-bootstrap-${node\_node\_name}.kubeconfig<br>  done<br>    <br>解释说明: 向 kubeconfig 写入的是 token，bootstrap 结束后 kube-controller-manager 为 kubelet 创建 client 和 server 证书；<br>    <br>查看 kubeadm 为各节点创建的 token：<br>[root@k8s-master01 work]\# kubeadm token list --kubeconfig ~/.kube/config<br>TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION               EXTRA GROUPS<br>0zqowl.aye8f834jtq9vm9t   23h       2019-06-19T16:50:43+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:k8s-node03<br>b46tq2.muab337gxwl0dsqn   23h       2019-06-19T16:50:43+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:k8s-node02<br>heh41x.foguhh1qa5crpzlq   23h       2019-06-19T16:50:42+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:k8s-node01<br>    <br>解释说明：<br>-&gt; token 有效期为 1 天，超期后将不能再被用来 boostrap kubelet，且会被 kube-controller-manager 的 tokencleaner 清理；<br>-&gt; kube-apiserver 接收 kubelet 的 bootstrap token 后，将请求的 user 设置为 system:bootstrap:&lt;Token ID&gt;，group 设置为 system:bootstrappers，<br>   后续将为这个 group 设置 ClusterRoleBinding；<br>    <br>查看各 token 关联的 Secret：<br>[root@k8s-master01 work]\# kubectl get secrets  -n kube-system|grep bootstrap-token<br>bootstrap-token-0zqowl                           bootstrap.kubernetes.io/token         7      88s<br>bootstrap-token-b46tq2                           bootstrap.kubernetes.io/token         7      88s<br>bootstrap-token-heh41x                           bootstrap.kubernetes.io/token         7      89s<br>    <br>3) 分发 bootstrap kubeconfig 文件到所有node节点<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_name in ${NODE\_NODE\_NAMES[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_name}"<br>    scp kubelet-bootstrap-${node\_node\_name}.kubeconfig root@${node\_node\_name}:/etc/kubernetes/kubelet-bootstrap.kubeconfig<br>  done<br>    <br>4) 创建和分发 kubelet 参数配置文件<br>从 v1.10 开始，部分 kubelet 参数需在配置文件中配置，kubelet --help 会提示：<br>DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag<br>    <br>创建 kubelet 参数配置文件模板：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# cat &gt; kubelet-config.yaml.template &lt;&lt;EOF<br>kind: KubeletConfiguration<br>apiVersion: kubelet.config.k8s.io/v1beta1<br>address: "\#\#NODE\_NODE\_IP\#\#"<br>staticPodPath: ""<br>syncFrequency: 1m<br>fileCheckFrequency: 20s<br>httpCheckFrequency: 20s<br>staticPodURL: ""<br>port: 10250<br>readOnlyPort: 0<br>rotateCertificates: true<br>serverTLSBootstrap: true<br>authentication:<br>  anonymous:<br>    enabled: false<br>  webhook:<br>    enabled: true<br>  x509:<br>    clientCAFile: "/etc/kubernetes/cert/ca.pem"<br>authorization:<br>  mode: Webhook<br>registryPullQPS: 0<br>registryBurst: 20<br>eventRecordQPS: 0<br>eventBurst: 20<br>enableDebuggingHandlers: true<br>enableContentionProfiling: true<br>healthzPort: 10248<br>healthzBindAddress: "\#\#NODE\_NODE\_IP\#\#"<br>clusterDomain: "${CLUSTER\_DNS\_DOMAIN}"<br>clusterDNS:<br>  - "${CLUSTER\_DNS\_SVC\_IP}"<br>nodeStatusUpdateFrequency: 10s<br>nodeStatusReportFrequency: 1m<br>imageMinimumGCAge: 2m<br>imageGCHighThresholdPercent: 85<br>imageGCLowThresholdPercent: 80<br>volumeStatsAggPeriod: 1m<br>kubeletCgroups: ""<br>systemCgroups: ""<br>cgroupRoot: ""<br>cgroupsPerQOS: true<br>cgroupDriver: cgroupfs<br>runtimeRequestTimeout: 10m<br>hairpinMode: promiscuous-bridge<br>maxPods: 220<br>podCIDR: "${CLUSTER\_CIDR}"<br>podPidsLimit: -1<br>resolvConf: /etc/resolv.conf<br>maxOpenFiles: 1000000<br>kubeAPIQPS: 1000<br>kubeAPIBurst: 2000<br>serializeImagePulls: false<br>evictionHard:<br>  memory.available:  "100Mi"<br>nodefs.available:  "10%"<br>nodefs.inodesFree: "5%"<br>imagefs.available: "15%"<br>evictionSoft: {}<br>enableControllerAttachDetach: true<br>failSwapOn: true<br>containerLogMaxSize: 20Mi<br>containerLogMaxFiles: 10<br>systemReserved: {}<br>kubeReserved: {}<br>systemReservedCgroup: ""<br>kubeReservedCgroup: ""<br>enforceNodeAllocatable: ["pods"]<br>EOF<br>    <br>解释说明:<br>-&gt; address：kubelet 安全端口（https，10250）监听的地址，不能为 127.0.0.1，否则 kube-apiserver、heapster 等不能调用 kubelet 的 API；<br>-&gt; readOnlyPort=0：关闭只读端口(默认 10255)，等效为未指定；<br>-&gt; authentication.anonymous.enabled：设置为 false，不允许匿名�访问 10250 端口；<br>-&gt; authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTP 证书认证；<br>-&gt; authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；<br>-&gt; 对于未通过 x509 证书和 webhook 认证的请求(kube-apiserver 或其他客户端)，将被拒绝，提示 Unauthorized；<br>-&gt; authroization.mode=Webhook：kubelet 使用 SubjectAccessReview API 查询 kube-apiserver 某 user、group 是否具有操作资源的权限(RBAC)；<br>-&gt; featureGates.RotateKubeletClientCertificate、featureGates.RotateKubeletServerCertificate：自动 rotate 证书，证书的有效期取决于<br>   kube-controller-manager 的 --experimental-cluster-signing-duration 参数；<br>-&gt; 需要 root 账户运行；<br>    <br>为各节点创建和分发 kubelet 配置文件：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    sed -e "s/\#\#NODE\_NODE\_IP\#\#/${node\_node\_ip}/" kubelet-config.yaml.template &gt; kubelet-config-${node\_node\_ip}.yaml.template<br>    scp kubelet-config-${node\_node\_ip}.yaml.template root@${node\_node\_ip}:/etc/kubernetes/kubelet-config.yaml<br>  done<br>    <br>5）创建和分发 kubelet systemd unit 文件<br>创建 kubelet systemd unit 文件模板：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# cat &gt; kubelet.service.template &lt;&lt;EOF<br>[Unit]<br>Description=Kubernetes Kubelet<br>Documentation=https://github.com/GoogleCloudPlatform/kubernetes<br>After=docker.service<br>Requires=docker.service<br>    <br>[Service]<br>WorkingDirectory=${K8S\_DIR}/kubelet<br>ExecStart=/opt/k8s/bin/kubelet \\\\<br>  --allow-privileged=true \\\\<br>  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \\\\<br>  --cert-dir=/etc/kubernetes/cert \\\\<br>  --cni-conf-dir=/etc/cni/net.d \\\\<br>  --container-runtime=docker \\\\<br>  --container-runtime-endpoint=unix:///var/run/dockershim.sock \\\\<br>  --root-dir=${K8S\_DIR}/kubelet \\\\<br>  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\<br>  --config=/etc/kubernetes/kubelet-config.yaml \\\\<br>  --hostname-override=\#\#NODE\_NODE\_NAME\#\# \\\\<br>  --pod-infra-container-image=registry.cn-beijing.aliyuncs.com/k8s\_images/pause-amd64:3.1 \\\\<br>  --image-pull-progress-deadline=15m \\\\<br>  --volume-plugin-dir=${K8S\_DIR}/kubelet/kubelet-plugins/volume/exec/ \\\\<br>  --logtostderr=true \\\\<br>  --v=2<br>Restart=always<br>RestartSec=5<br>StartLimitInterval=0<br>    <br>[Install]<br>WantedBy=multi-user.target<br>EOF<br>    <br>解释说明：<br>-&gt; 如果设置了 --hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；<br>-&gt; --bootstrap-kubeconfig：指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；<br>-&gt; K8S approve kubelet 的 csr 请求后，在 --cert-dir 目录创建证书和私钥文件，然后写入 --kubeconfig 文件；<br>-&gt; --pod-infra-container-image 不使用 redhat 的 pod-infrastructure:latest 镜像，它不能回收容器的僵尸；<br>    <br>为各节点创建和分发 kubelet systemd unit 文件：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_name in ${NODE\_NODE\_NAMES[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_name}"<br>    sed -e "s/\#\#NODE\_NODE\_NAME\#\#/${node\_node\_name}/" kubelet.service.template &gt; kubelet-${node\_node\_name}.service<br>    scp kubelet-${node\_node\_name}.service root@${node\_node\_name}:/etc/systemd/system/kubelet.service<br>  done<br>    <br>6）Bootstrap Token Auth 和授予权限<br>-&gt; kubelet启动时查找--kubeletconfig参数对应的文件是否存在，如果不存在则使用 --bootstrap-kubeconfig 指定的 kubeconfig 文件向 kube-apiserver 发送证书签名请求 (CSR)。<br>-&gt; kube-apiserver 收到 CSR 请求后，对其中的 Token 进行认证，认证通过后将请求的 user 设置为 system:bootstrap:&lt;Token ID&gt;，group 设置为 system:bootstrappers，<br>   这一过程称为 Bootstrap Token Auth。<br>-&gt; 默认情况下，这个 user 和 group 没有创建 CSR 的权限，kubelet 启动失败，错误日志如下：<br>   \# journalctl -u kubelet -a |grep -A 2 'certificatesigningrequests'<br>   May 9 22:48:41 k8s-master01 kubelet[128468]: I0526 22:48:41.798230  128468 certificate\_manager.go:366] Rotating certificates<br>   May 9 22:48:41 k8s-master01 kubelet[128468]: E0526 22:48:41.801997  128468 certificate\_manager.go:385] Failed while requesting a signed certificate from the master: cannot cre<br>   ate certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User "system:bootstrap:82jfrm" cannot create resource "certificatesigningrequests" i<br>   n API group "certificates.k8s.io" at the cluster scope<br>   May 9 22:48:42 k8s-master01 kubelet[128468]: E0526 22:48:42.044828  128468 kubelet.go:2244] node "k8s-master01" not found<br>   May 9 22:48:42 k8s-master01 kubelet[128468]: E0526 22:48:42.078658  128468 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/kubelet.go:442: Failed to list \*v1.Service: Unauthor<br>   ized<br>   May 9 22:48:42 k8s-master01 kubelet[128468]: E0526 22:48:42.079873  128468 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to list \*v1.Node: Unauthorize<br>   d<br>   May 9 22:48:42 k8s-master01 kubelet[128468]: E0526 22:48:42.082683  128468 reflector.go:126] k8s.io/client-go/informers/factory.go:133: Failed to list \*v1beta1.CSIDriver: Unau<br>   thorized<br>   May 9 22:48:42 k8s-master01 kubelet[128468]: E0526 22:48:42.084473  128468 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list \*v1.Pod: Unau<br>   thorized<br>   May 9 22:48:42 k8s-master01 kubelet[128468]: E0526 22:48:42.088466  128468 reflector.go:126] k8s.io/client-go/informers/factory.go:133: Failed to list \*v1beta1.RuntimeClass: U<br>   nauthorized<br>    <br>   解决办法是：创建一个 clusterrolebinding，将 group system:bootstrappers 和 clusterrole system:node-bootstrapper 绑定：<br>   \# kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers<br>    <br>7) 启动 kubelet 服务<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "mkdir -p ${K8S\_DIR}/kubelet/kubelet-plugins/volume/exec/"<br>    ssh root@${node\_node\_ip} "/usr/sbin/swapoff -a"<br>    ssh root@${node\_node\_ip} "systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet"<br>  done<br>    <br>解释说明：<br>-&gt; 启动服务前必须先创建工作目录；<br>-&gt; 关闭 swap 分区，否则 kubelet 会启动失败 (使用"journalctl -u kubelet |tail"命令查看错误日志)<br>    <br>kubelet 启动后使用 --bootstrap-kubeconfig 向 kube-apiserver 发送 CSR 请求，<br>当这个 CSR 被 approve 后，kube-controller-manager 为 kubelet 创建 TLS 客户端证书、私钥和 --kubeletconfig 文件。<br>    <br>注意：kube-controller-manager 需要配置 --cluster-signing-cert-file 和 --cluster-signing-key-file 参数，才会为 TLS Bootstrap 创建证书和私钥。<br>    <br>[root@k8s-master01 work]\# kubectl get csr<br>NAME        AGE    REQUESTOR                 CONDITION<br>csr-4wk6q   108s   system:bootstrap:0zqowl   Pending<br>csr-mjtl5   110s   system:bootstrap:heh41x   Pending<br>csr-rfz27   109s   system:bootstrap:b46tq2   Pending<br>    <br>[root@k8s-master01 work]\# kubectl get nodes<br>No resources found.<br>    <br>此时三个node节点的csr均处于 pending 状态；<br>    <br>8）自动 approve CSR 请求<br>创建三个 ClusterRoleBinding，分别用于自动 approve client、renew client、renew server 证书：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cat &gt; csr-crb.yaml &lt;&lt;EOF<br> \# Approve all CSRs for the group "system:bootstrappers"<br> kind: ClusterRoleBinding<br> apiVersion: rbac.authorization.k8s.io/v1<br> metadata:<br>   name: auto-approve-csrs-for-group<br> subjects:<br> - kind: Group<br>   name: system:bootstrappers<br>   apiGroup: rbac.authorization.k8s.io<br> roleRef:<br>   kind: ClusterRole<br>   name: system:certificates.k8s.io:certificatesigningrequests:nodeclient<br>   apiGroup: rbac.authorization.k8s.io<br>---<br> \# To let a node of the group "system:nodes" renew its own credentials<br> kind: ClusterRoleBinding<br> apiVersion: rbac.authorization.k8s.io/v1<br> metadata:<br>   name: node-client-cert-renewal<br> subjects:<br> - kind: Group<br>   name: system:nodes<br>   apiGroup: rbac.authorization.k8s.io<br> roleRef:<br>   kind: ClusterRole<br>   name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient<br>   apiGroup: rbac.authorization.k8s.io<br>---<br>\# A ClusterRole which instructs the CSR approver to approve a node requesting a<br>\# serving cert matching its client cert.<br>kind: ClusterRole<br>apiVersion: rbac.authorization.k8s.io/v1<br>metadata:<br>  name: approve-node-server-renewal-csr<br>rules:<br>- apiGroups: ["certificates.k8s.io"]<br>  resources: ["certificatesigningrequests/selfnodeserver"]<br>  verbs: ["create"]<br>---<br> \# To let a node of the group "system:nodes" renew its own server credentials<br> kind: ClusterRoleBinding<br> apiVersion: rbac.authorization.k8s.io/v1<br> metadata:<br>   name: node-server-cert-renewal<br> subjects:<br> - kind: Group<br>   name: system:nodes<br>   apiGroup: rbac.authorization.k8s.io<br> roleRef:<br>   kind: ClusterRole<br>   name: approve-node-server-renewal-csr<br>   apiGroup: rbac.authorization.k8s.io<br>EOF<br>    <br>解释说明：<br>-&gt; auto-approve-csrs-for-group：自动 approve node 的第一次 CSR； 注意第一次 CSR 时，请求的 Group 为 system:bootstrappers；<br>-&gt; node-client-cert-renewal：自动 approve node 后续过期的 client 证书，自动生成的证书 Group 为 system:nodes;<br>-&gt; node-server-cert-renewal：自动 approve node 后续过期的 server 证书，自动生成的证书 Group 为 system:nodes;<br>    <br>执行创建：<br>[root@k8s-master01 work]\# kubectl apply -f csr-crb.yaml<br>    <br>查看 kubelet 的情况<br>    <br>需要耐心等待一段时间(1-10 分钟)，三个节点的 CSR 都被自动 approved（测试时等待了很长一段时间才被自动approved）<br>[root@k8s-master01 work]\# kubectl get csr<br>NAME        AGE     REQUESTOR                 CONDITION<br>csr-4m4hc   37s     system:node:k8s-node01    Pending<br>csr-4wk6q   7m29s   system:bootstrap:0zqowl   Approved,Issued<br>csr-h8hq6   36s     system:node:k8s-node02    Pending<br>csr-mjtl5   7m31s   system:bootstrap:heh41x   Approved,Issued<br>csr-rfz27   7m30s   system:bootstrap:b46tq2   Approved,Issued<br>csr-t9p6n   36s     system:node:k8s-node03    Pending<br>    <br>注意：<br>Pending 的 CSR 用于创建 kubelet server 证书，需要手动 approve，后续会说到这个。<br>    <br>此时发现所有node节点状态均为"ready"：<br>[root@k8s-master01 work]\# kubectl get nodes<br>NAME         STATUS   ROLES    AGE     VERSION<br>k8s-node01   Ready    &lt;none&gt;   3m      v1.14.2<br>k8s-node02   Ready    &lt;none&gt;   3m      v1.14.2<br>k8s-node03   Ready    &lt;none&gt;   2m59s   v1.14.2<br>    <br>kube-controller-manager 为各node节点生成了 kubeconfig 文件和公私钥（如下在node节点上执行）：<br>[root@k8s-node01 ~]\# ls -l /etc/kubernetes/kubelet.kubeconfig<br>-rw------- 1 root root 2310 Jun 18 17:09 /etc/kubernetes/kubelet.kubeconfig<br>    <br>[root@k8s-node01 ~]\# ls -l /etc/kubernetes/cert/|grep kubelet<br>-rw------- 1 root root 1273 Jun 18 17:16 kubelet-client-2019-06-18-17-16-31.pem<br>lrwxrwxrwx 1 root root   59 Jun 18 17:16 kubelet-client-current.pem -&gt; /etc/kubernetes/cert/kubelet-client-2019-06-18-17-16-31.pem<br>    <br>注意：此时还没有自动生成 kubelet server 证书；<br>    <br>9）手动 approve server cert csr<br>基于安全性考虑，CSR approving controllers 不会自动 approve kubelet server 证书签名请求，需要手动 approve：<br>    <br>[root@k8s-master01 work]\# kubectl get csr<br>NAME        AGE    REQUESTOR                 CONDITION<br>csr-4m4hc   6m4s   system:node:k8s-node01    Pending<br>csr-4wk6q   12m    system:bootstrap:0zqowl   Approved,Issued<br>csr-h8hq6   6m3s   system:node:k8s-node02    Pending<br>csr-mjtl5   12m    system:bootstrap:heh41x   Approved,Issued<br>csr-rfz27   12m    system:bootstrap:b46tq2   Approved,Issued<br>csr-t9p6n   6m3s   system:node:k8s-node03    Pending<br>    <br>记住上面执行结果为"Pending"的对应的csr的NAME名称，然后对这些csr进行手动approve<br>[root@k8s-master01 work]\# kubectl certificate approve csr-4m4hc<br>certificatesigningrequest.certificates.k8s.io/csr-4m4hc approved<br>    <br>[root@k8s-master01 work]\# kubectl certificate approve csr-h8hq6<br>certificatesigningrequest.certificates.k8s.io/csr-h8hq6 approved<br>    <br>[root@k8s-master01 work]\# kubectl certificate approve csr-t9p6n<br>certificatesigningrequest.certificates.k8s.io/csr-t9p6n approved<br>    <br>再次查看csr，发现所有的CSR都为approved了<br>[root@k8s-master01 work]\# kubectl get csr<br>NAME        AGE     REQUESTOR                 CONDITION<br>csr-4m4hc   7m46s   system:node:k8s-node01    Approved,Issued<br>csr-4wk6q   14m     system:bootstrap:0zqowl   Approved,Issued<br>csr-h8hq6   7m45s   system:node:k8s-node02    Approved,Issued<br>csr-mjtl5   14m     system:bootstrap:heh41x   Approved,Issued<br>csr-rfz27   14m     system:bootstrap:b46tq2   Approved,Issued<br>csr-t9p6n   7m45s   system:node:k8s-node03    Approved,Issued<br>    <br>再次到node节点上查看，发现已经自动生成 kubelet server 证书；<br>[root@k8s-node01 ~]\# ls -l /etc/kubernetes/cert/kubelet-\*<br>-rw------- 1 root root 1273 Jun 18 17:16 /etc/kubernetes/cert/kubelet-client-2019-06-18-17-16-31.pem<br>lrwxrwxrwx 1 root root   59 Jun 18 17:16 /etc/kubernetes/cert/kubelet-client-current.pem -&gt; /etc/kubernetes/cert/kubelet-client-2019-06-18-17-16-31.pem<br>-rw------- 1 root root 1317 Jun 18 17:23 /etc/kubernetes/cert/kubelet-server-2019-06-18-17-23-13.pem<br>lrwxrwxrwx 1 root root   59 Jun 18 17:23 /etc/kubernetes/cert/kubelet-server-current.pem -&gt; /etc/kubernetes/cert/kubelet-server-2019-06-18-17-23-13.pem<br>    <br>10）kubelet 提供的 API 接口<br>kubelet 启动后监听多个端口，用于接收 kube-apiserver 或其它客户端发送的请求：<br>    <br>在node节点执行下面命令<br>[root@k8s-node01 ~]\# netstat -lnpt|grep kubelet<br>tcp        0      0 127.0.0.1:40831         0.0.0.0:\*               LISTEN      24468/kubelet   <br>tcp        0      0 172.16.60.244:10248     0.0.0.0:\*               LISTEN      24468/kubelet   <br>tcp        0      0 172.16.60.244:10250     0.0.0.0:\*               LISTEN      24468/kubelet<br>    <br>解释说明：<br>-&gt; 10248: healthz http服务端口，即健康检查服务的端口<br>-&gt; 10250: kubelet服务监听的端口,api会检测他是否存活。即https服务，访问该端口时需要认证和授权（即使访问/healthz也需要）；<br>-&gt; 10255：只读端口，可以不用验证和授权机制，直接访问。这里配置"readOnlyPort: 0"表示未开启只读端口10255；如果配置"readOnlyPort: 10255"则打开10255端口<br>-&gt; 从 K8S v1.10 开始，去除了 --cadvisor-port 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp; API。<br>    <br>例如执行"kubectl exec -it nginx-ds-5aedg -- sh"命令时，kube-apiserver会向 kubelet 发送如下请求：<br>    POST /exec/default/nginx-ds-5aedg/my-nginx?command=sh&amp;input=1&amp;output=1&amp;tty=1<br>    <br>kubelet 接收 10250 端口的 https 请求，可以访问如下资源：<br>-&gt; /pods、/runningpods<br>-&gt; /metrics、/metrics/cadvisor、/metrics/probes<br>-&gt; /spec<br>-&gt; /stats、/stats/container<br>-&gt; /logs<br>-&gt; /run/、/exec/, /attach/, /portForward/, /containerLogs/<br>    <br>由于关闭了匿名认证，同时开启了webhook 授权，所有访问10250端口https API的请求都需要被认证和授权。<br>预定义的 ClusterRole system:kubelet-api-admin 授予访问 kubelet 所有 API 的权限(kube-apiserver 使用的 kubernetes 证书 User 授予了该权限)：<br>    <br>[root@k8s-master01 work]\# kubectl describe clusterrole system:kubelet-api-admin<br>Name:         system:kubelet-api-admin<br>Labels:       kubernetes.io/bootstrapping=rbac-defaults<br>Annotations:  rbac.authorization.kubernetes.io/autoupdate: true<br>PolicyRule:<br>  Resources      Non-Resource URLs  Resource Names  Verbs<br>  ---------      -----------------  --------------  -----<br>  nodes/log      []                 []              [\*]<br>  nodes/metrics  []                 []              [\*]<br>  nodes/proxy    []                 []              [\*]<br>  nodes/spec     []                 []              [\*]<br>  nodes/stats    []                 []              [\*]<br>  nodes          []                 []              [get list watch proxy]<br>    <br>11) kubelet api 认证和授权<br>kubelet 配置了如下认证参数：<br>-&gt; authentication.anonymous.enabled：设置为 false，不允许匿名�访问 10250 端口；<br>-&gt; authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTPs 证书认证；<br>-&gt; authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；<br>    <br>同时配置了如下授权参数：<br>-&gt; authroization.mode=Webhook：开启 RBAC 授权；<br>    <br>kubelet 收到请求后，使用 clientCAFile 对证书签名进行认证，或者查询 bearer token 是否有效。如果两者都没通过，则拒绝请求，提示 Unauthorized：<br>[root@k8s-master01 work]\# curl -s --cacert /etc/kubernetes/cert/ca.pem https://172.16.60.244:10250/metrics<br>Unauthorized<br>    <br>[root@k8s-master01 work]\# curl -s --cacert /etc/kubernetes/cert/ca.pem -H "Authorization: Bearer 123456" https://172.16.60.244:10250/metrics<br>Unauthorized<br>    <br>通过认证后，kubelet 使用 SubjectAccessReview API 向 kube-apiserver 发送请求，查询证书或 token 对应的 user、group 是否有操作资源的权限(RBAC)；<br>    <br>下面进行证书认证和授权：<br>    <br>\# 权限不足的证书；<br>[root@k8s-master01 work]\# curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /etc/kubernetes/cert/kube-controller-manager.pem --key /etc/kubernetes/cert/kube-controller-manager-key.pem https://172.16.60.244:10250/metrics<br>Forbidden (user=system:kube-controller-manager, verb=get, resource=nodes, subresource=metrics)<br>    <br>\# 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；<br>[root@k8s-master01 work]\# curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://172.16.60.244:10250/metrics|head<br>\# HELP apiserver\_audit\_event\_total Counter of audit events generated and sent to the audit backend.<br>\# TYPE apiserver\_audit\_event\_total counter<br>apiserver\_audit\_event\_total 0<br>\# HELP apiserver\_audit\_requests\_rejected\_total Counter of apiserver requests rejected due to an error in audit logging backend.<br>\# TYPE apiserver\_audit\_requests\_rejected\_total counter<br>apiserver\_audit\_requests\_rejected\_total 0<br>\# HELP apiserver\_client\_certificate\_expiration\_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.<br>\# TYPE apiserver\_client\_certificate\_expiration\_seconds histogram<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="0"} 0<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="1800"} 0<br>    <br>注意：--cacert、--cert、--key 的参数值必须是文件路径，否则返回 401 Unauthorized；<br>    <br>bear token 认证和授权<br>创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限：<br>[root@k8s-master01 work]\# kubectl create sa kubelet-api-test<br>[root@k8s-master01 work]\# kubectl create clusterrolebinding kubelet-api-test --clusterrole=system:kubelet-api-admin --serviceaccount=default:kubelet-api-test<br>[root@k8s-master01 work]\# SECRET=$(kubectl get secrets | grep kubelet-api-test | awk '{print $1}')<br>[root@k8s-master01 work]\# TOKEN=$(kubectl describe secret ${SECRET} | grep -E '^token' | awk '{print $2}')<br>[root@k8s-master01 work]\# echo ${TOKEN}<br>eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Imt1YmVsZXQtYXBpLXRlc3QtdG9rZW4tanRyMnEiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoia3ViZWxldC1hcGktdGVzdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImRjYjljZTE0LTkxYWMtMTFlOS05MGQ0LTAwNTA1NmFjN2M4MSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0Omt1YmVsZXQtYXBpLXRlc3QifQ.i\_uVqjOUMLdG4lDURfhxFDOtM2addxgEquQTcpOLP\_5g6UI-MjvE5jHem\_Q8OtMwFs5tqlCvKJHN2IdfsRiKk\_mBe\_ysLQsNEoHDclZwHRVN6X84Y62q49y-ArT12YlSpfWWenw-2GawsTmORbz7AYYaU5-kgqMk95mMx57ic8uwvJYlilw4JCnkMON5ESOmgAOg30uVvsBiQVkkYTwGtAG5Tah9wADujQttBjjDOlGntpGHxj-HmZO2GivDgdrbs\_UNvhzGt2maDlpP13qYv8zKiBGpSbiWOAk\_olsFKQ5-dIrn04NCbh9Kkyyh9JccMSuvePaj-lgTWj5zdUfRHw<br>    <br>这时，再接着进行kubelet请求<br>[root@k8s-master01 work]\# curl -s --cacert /etc/kubernetes/cert/ca.pem -H "Authorization: Bearer ${TOKEN}" https://172.16.60.244:10250/metrics|head<br>\# HELP apiserver\_audit\_event\_total Counter of audit events generated and sent to the audit backend.<br>\# TYPE apiserver\_audit\_event\_total counter<br>apiserver\_audit\_event\_total 0<br>\# HELP apiserver\_audit\_requests\_rejected\_total Counter of apiserver requests rejected due to an error in audit logging backend.<br>\# TYPE apiserver\_audit\_requests\_rejected\_total counter<br>apiserver\_audit\_requests\_rejected\_total 0<br>\# HELP apiserver\_client\_certificate\_expiration\_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.<br>\# TYPE apiserver\_client\_certificate\_expiration\_seconds histogram<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="0"} 0<br>apiserver\_client\_certificate\_expiration\_seconds\_bucket{le="1800"} 0<br>    <br>12）cadvisor 和 metrics<br>cadvisor 是内嵌在 kubelet 二进制中的，统计所在节点各容器的资源(CPU、内存、磁盘、网卡)使用情况的服务。<br>浏览器访问https://172.16.60.244:10250/metrics 和 https://172.16.60.244:10250/metrics/cadvisor 分别返回 kubelet 和 cadvisor 的 metrics。<br>    <br>注意：<br>-&gt; kubelet.config.json 设置 authentication.anonymous.enabled 为 false，不允许匿名证书访问 10250 的 https 服务；<br>-&gt; 参考下面的"浏览器访问kube-apiserver安全端口"，创建和导入相关证书，然后就可以在浏览器里成功访问kube-apiserver和上面的kubelet的10250端口了。<br>    <br>需要通过证书方式访问kubelet的10250端口<br>[root@k8s-master01 ~]\# curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://172.16.60.244:10250/metrics<br>    <br>[root@k8s-master01 ~]\# curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://172.16.60.244:10250/metrics/cadvisor<br>   <br>13）获取 kubelet 的配置<br>从 kube-apiserver 获取各节点 kubelet 的配置：<br>如果发现没有jq命令（json处理工具），可以直接yum安装jq：<br>[root@k8s-master01 ~]\# yum install -y jq<br>   <br>使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；<br>[root@k8s-master01 ~]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 ~]\# curl -sSL --cacert /etc/kubernetes/cert/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem ${KUBE\_APISERVER}/api/v1/nodes/k8s-node01/proxy/configz | jq '.kubeletconfig|.kind="KubeletConfiguration"|.apiVersion="kubelet.config.k8s.io/v1beta1"'<br>{<br>  "syncFrequency": "1m0s",<br>  "fileCheckFrequency": "20s",<br>  "httpCheckFrequency": "20s",<br>  "address": "172.16.60.244",<br>  "port": 10250,<br>  "rotateCertificates": true,<br>  "serverTLSBootstrap": true,<br>  "authentication": {<br>    "x509": {<br>      "clientCAFile": "/etc/kubernetes/cert/ca.pem"<br>    },<br>    "webhook": {<br>      "enabled": true,<br>      "cacheTTL": "2m0s"<br>    },<br>    "anonymous": {<br>      "enabled": false<br>    }<br>  },<br>  "authorization": {<br>    "mode": "Webhook",<br>    "webhook": {<br>      "cacheAuthorizedTTL": "5m0s",<br>      "cacheUnauthorizedTTL": "30s"<br>    }<br>  },<br>  "registryPullQPS": 0,<br>  "registryBurst": 20,<br>  "eventRecordQPS": 0,<br>  "eventBurst": 20,<br>  "enableDebuggingHandlers": true,<br>  "enableContentionProfiling": true,<br>  "healthzPort": 10248,<br>  "healthzBindAddress": "172.16.60.244",<br>  "oomScoreAdj": -999,<br>  "clusterDomain": "cluster.local",<br>  "clusterDNS": [<br>    "10.254.0.2"<br>  ],<br>  "streamingConnectionIdleTimeout": "4h0m0s",<br>  "nodeStatusUpdateFrequency": "10s",<br>  "nodeStatusReportFrequency": "1m0s",<br>  "nodeLeaseDurationSeconds": 40,<br>  "imageMinimumGCAge": "2m0s",<br>  "imageGCHighThresholdPercent": 85,<br>  "imageGCLowThresholdPercent": 80,<br>  "volumeStatsAggPeriod": "1m0s",<br>  "cgroupsPerQOS": true,<br>  "cgroupDriver": "cgroupfs",<br>  "cpuManagerPolicy": "none",<br>  "cpuManagerReconcilePeriod": "10s",<br>  "runtimeRequestTimeout": "10m0s",<br>  "hairpinMode": "promiscuous-bridge",<br>  "maxPods": 220,<br>  "podCIDR": "172.30.0.0/16",<br>  "podPidsLimit": -1,<br>  "resolvConf": "/etc/resolv.conf",<br>  "cpuCFSQuota": true,<br>  "cpuCFSQuotaPeriod": "100ms",<br>  "maxOpenFiles": 1000000,<br>  "contentType": "application/vnd.kubernetes.protobuf",<br>  "kubeAPIQPS": 1000,<br>  "kubeAPIBurst": 2000,<br>  "serializeImagePulls": false,<br>  "evictionHard": {<br>    "memory.available": "100Mi"<br>  },<br>  "evictionPressureTransitionPeriod": "5m0s",<br>  "enableControllerAttachDetach": true,<br>  "makeIPTablesUtilChains": true,<br>  "iptablesMasqueradeBit": 14,<br>  "iptablesDropBit": 15,<br>  "failSwapOn": true,<br>  "containerLogMaxSize": "20Mi",<br>  "containerLogMaxFiles": 10,<br>  "configMapAndSecretChangeDetectionStrategy": "Watch",<br>  "enforceNodeAllocatable": [<br>    "pods"<br>  ],<br>  "kind": "KubeletConfiguration",<br>  "apiVersion": "kubelet.config.k8s.io/v1beta1"<br>}<br>  <br>或者直接执行下面语句：（https://172.16.60.250:8443 就是变量${KUBE\_APISERVER}）<br>[root@k8s-master01 ~]\# curl -sSL --cacert /etc/kubernetes/cert/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://172.16.60.250:8443/api/v1/nodes/k8s-node01/proxy/configz | jq '.kubeletconfig|.kind="KubeletConfiguration"|.apiVersion="kubelet.config.k8s.io/v1beta1"'<br>[root@k8s-master01 ~]\# curl -sSL --cacert /etc/kubernetes/cert/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://172.16.60.250:8443/api/v1/nodes/k8s-node02/proxy/configz | jq '.kubeletconfig|.kind="KubeletConfiguration"|.apiVersion="kubelet.config.k8s.io/v1beta1"'<br>[root@k8s-master01 ~]\# curl -sSL --cacert /etc/kubernetes/cert/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://172.16.60.250:8443/api/v1/nodes/k8s-node03/proxy/configz | jq '.kubeletconfig|.kind="KubeletConfiguration"|.apiVersion="kubelet.config.k8s.io/v1beta1"' |


9.3 - 浏览器访问kube-apiserver等安全端口，创建和导入证书的做法

浏览器访问 kube-apiserver 的安全端口 6443 （代理端口是8443）时，提示证书不被信任：

![](images/463779352D294323BB1B497974F1589A2-1718777678.png)

![](images/7D84E11BD0BC491EB4D5917BA358D8382-1782663539.png)

这是因为 kube-apiserver 的 server 证书是我们创建的根证书 ca.pem 签名的，需要将根证书 ca.pem 导入操作系统，并设置永久信任。

这里说下Mac OS系统客户机上导入证书的方法:

1）点击Mac本上的"钥匙串访问" -> "系统" -> "证书" -> "kebernetes"(双击里面的"信任"，改成"始终信任"),如下图：

![](images/7E793D8BE09C45D1983547D509E2C7276-1384010838.png)

清除浏览器缓存，再次访问，发现证书已经被信任了！(红色感叹号已经消失了)

![](images/3DEEBD5CB4794D4BAB181656AA9838DB0-1190232596.png)

2）需要给浏览器生成一个 client 证书，访问 apiserver 的 6443 https 端口时使用。这里使用部署 kubectl 命令行工具时创建的 admin 证书、私钥和上面的 ca 证书，创建一个浏览器可以使用 PKCS#12/PFX 格式的证书:

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7 | [root@k8s-master01 ~]\# cd /opt/k8s/work/<br>[root@k8s-master01 work]\# openssl pkcs12 -export -out admin.pfx -inkey admin-key.pem -in admin.pem -certfile ca.pem<br>Enter Export Password:                      \# 这里输入自己设定的任意密码，比如"123456"<br>Verifying - Enter Export Password:          \# 确认密码: 123456<br> <br>[root@k8s-master01 work]\# ll admin.pfx<br>-rw-r--r-- 1 root root 3613 Jun 23 23:56 admin.pfx |


将在k8s-master01服务器上生成的client证书admin.pfx拷贝到Mac本机，导入到"钥匙串访问" -> "系统" -> "证书" 里面 (导入时会提示输入admin.pfx证书的密码，即"123456")，如下图:

![](images/46C778CD4238451FB4242BC17FC3046833-856394933.png)

清除浏览器历史记录，一定要重启浏览器，接着访问apiserver地址，接着会提示选择一个浏览器证书，这里选中上面导入的"admin.pfx", 然后再次访问apiserver，发现相应的metrics数据就成功显示出来了！！(注意，如果失败了。则可以删除证书，然后重新生成，重新导入再跟着操作步骤来一遍，清除浏览器缓存，重启浏览器，选择导入的证书，再次访问即可！）

![](images/774836ACC75C44C8A6D2D892B16AAD8694-684215464.png)

![](images/5094A2FCF5B046EF99D6632B7443F3CF5-1516605584.png)

同样的，再上面apiserver访问的client证书导入到本地浏览器后，再访问kubelet的10250端口的metric时，也会提示选择导入的证书"admin.pfx"，然后就会正常显示对应的metrics数据了。（k8s集群的其他组件metrics的https证书方式方式同理！）

![](images/97D1BCA3C28A4E11AF248D6F0C95DA7889-189114630.png)

![](images/0E42BB72BE924F29A312B8DCCE1C2699030-70394262.png)

9.4 - 部署 kube-proxy 组件

kube-proxy运行在所有的node节点上，它监听apiserver中service和endpoint的变化情况，创建路由规则以提供服务IP和负载均衡功能。下面部署命令均在k8s-master01节点上执行，然后远程分发文件和执行命令。

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238 | 1）下载和分发 kube-proxy 二进制文件<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    scp kubernetes/server/bin/kube-proxy root@${node\_node\_ip}:/opt/k8s/bin/<br>    ssh root@${node\_node\_ip} "chmod +x /opt/k8s/bin/\*"<br>  done<br> <br>2) 创建 kube-proxy 证书<br>创建证书签名请求：<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cat &gt; kube-proxy-csr.json &lt;&lt;EOF<br>{<br>  "CN": "system:kube-proxy",<br>  "key": {<br>    "algo": "rsa",<br>    "size": 2048<br>  },<br>  "names": [<br>    {<br>      "C": "CN",<br>      "ST": "BeiJing",<br>      "L": "BeiJing",<br>      "O": "k8s",<br>      "OU": "4Paradigm"<br>    }<br>  ]<br>}<br>EOF<br> <br>注意：<br>CN：指定该证书的 User 为 system:kube-proxy；<br>预定义的 RoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；<br>该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空；<br> <br>生成证书和私钥：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cfssl gencert -ca=/opt/k8s/work/ca.pem \\<br>  -ca-key=/opt/k8s/work/ca-key.pem \\<br>  -config=/opt/k8s/work/ca-config.json \\<br>  -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy<br> <br>[root@k8s-master01 work]\# ll kube-proxy\*<br>-rw-r--r-- 1 root root 1013 Jun 24 20:21 kube-proxy.csr<br>-rw-r--r-- 1 root root  218 Jun 24 20:21 kube-proxy-csr.json<br>-rw------- 1 root root 1679 Jun 24 20:21 kube-proxy-key.pem<br>-rw-r--r-- 1 root root 1411 Jun 24 20:21 kube-proxy.pem<br> <br>3）创建和分发 kubeconfig 文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# kubectl config set-cluster kubernetes \\<br>  --certificate-authority=/opt/k8s/work/ca.pem \\<br>  --embed-certs=true \\<br>  --server=${KUBE\_APISERVER} \\<br>  --kubeconfig=kube-proxy.kubeconfig<br> <br>[root@k8s-master01 work]\# kubectl config set-credentials kube-proxy \\<br>  --client-certificate=kube-proxy.pem \\<br>  --client-key=kube-proxy-key.pem \\<br>  --embed-certs=true \\<br>  --kubeconfig=kube-proxy.kubeconfig<br> <br>[root@k8s-master01 work]\# kubectl config set-context default \\<br>  --cluster=kubernetes \\<br>  --user=kube-proxy \\<br>  --kubeconfig=kube-proxy.kubeconfig<br> <br>[root@k8s-master01 work]\# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig<br> <br>注意：--embed-certs=true：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl-proxy.kubeconfig 文件中(不加时，写入的是证书文件路径)；<br> <br>分发 kubeconfig 文件：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_name in ${NODE\_NODE\_NAMES[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_name}"<br>    scp kube-proxy.kubeconfig root@${node\_node\_name}:/etc/kubernetes/<br>  done<br> <br>4）创建 kube-proxy 配置文件<br>从 v1.10 开始，kube-proxy 部分参数可以配置文件中配置。可以使用 --write-config-to 选项生成该配置文件。<br>创建 kube-proxy config 文件模板：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cat &gt; kube-proxy-config.yaml.template &lt;&lt;EOF<br>kind: KubeProxyConfiguration<br>apiVersion: kubeproxy.config.k8s.io/v1alpha1<br>clientConnection:<br>  burst: 200<br>  kubeconfig: "/etc/kubernetes/kube-proxy.kubeconfig"<br>  qps: 100<br>bindAddress: \#\#NODE\_NODE\_IP\#\#<br>healthzBindAddress: \#\#NODE\_NODE\_IP\#\#:10256<br>metricsBindAddress: \#\#NODE\_NODE\_IP\#\#:10249<br>enableProfiling: true<br>clusterCIDR: ${CLUSTER\_CIDR}<br>hostnameOverride: \#\#NODE\_NODE\_NAME\#\#<br>mode: "ipvs"<br>portRange: ""<br>kubeProxyIPTablesConfiguration:<br>  masqueradeAll: false<br>kubeProxyIPVSConfiguration:<br>  scheduler: rr<br>  excludeCIDRs: []<br>EOF<br> <br>注意：<br>bindAddress: 监听地址；<br>clientConnection.kubeconfig: 连接 apiserver 的 kubeconfig 文件；<br>clusterCIDR: kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；<br>hostnameOverride: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则；<br>mode: 使用 ipvs 模式；<br> <br>为各节点创建和分发 kube-proxy 配置文件：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for (( i=0; i &lt; 3; i++ ))<br>  do<br>    echo "&gt;&gt;&gt; ${NODE\_NODE\_NAMES[i]}"<br>    sed -e "s/\#\#NODE\_NODE\_NAME\#\#/${NODE\_NODE\_NAMES[i]}/" -e "s/\#\#NODE\_NODE\_IP\#\#/${NODE\_NODE\_IPS[i]}/" kube-proxy-config.yaml.template &gt; kube-proxy-config-${NODE\_NODE\_NAMES[i]}.yaml.template<br>    scp kube-proxy-config-${NODE\_NODE\_NAMES[i]}.yaml.template root@${NODE\_NODE\_NAMES[i]}:/etc/kubernetes/kube-proxy-config.yaml<br>  done<br> <br>[root@k8s-master01 work]\# ll kube-proxy-config-k8s-node0\*<br>-rw-r--r-- 1 root root 500 Jun 24 20:27 kube-proxy-config-k8s-node01.yaml.template<br>-rw-r--r-- 1 root root 500 Jun 24 20:27 kube-proxy-config-k8s-node02.yaml.template<br>-rw-r--r-- 1 root root 500 Jun 24 20:27 kube-proxy-config-k8s-node03.yaml.template<br> <br>5）创建和分发 kube-proxy systemd unit 文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# cat &gt; kube-proxy.service &lt;&lt;EOF<br>[Unit]<br>Description=Kubernetes Kube-Proxy Server<br>Documentation=https://github.com/GoogleCloudPlatform/kubernetes<br>After=network.target<br> <br>[Service]<br>WorkingDirectory=${K8S\_DIR}/kube-proxy<br>ExecStart=/opt/k8s/bin/kube-proxy \\\\<br>  --config=/etc/kubernetes/kube-proxy-config.yaml \\\\<br>  --logtostderr=true \\\\<br>  --v=2<br>Restart=on-failure<br>RestartSec=5<br>LimitNOFILE=65536<br> <br>[Install]<br>WantedBy=multi-user.target<br>EOF<br> <br>分发 kube-proxy systemd unit 文件：<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_name in ${NODE\_NODE\_NAMES[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_name}"<br>    scp kube-proxy.service root@${node\_node\_name}:/etc/systemd/system/<br>  done<br> <br>6）启动 kube-proxy 服务<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "mkdir -p ${K8S\_DIR}/kube-proxy"<br>    ssh root@${node\_node\_ip} "modprobe ip\_vs\_rr"<br>    ssh root@${node\_node\_ip} "systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy"<br>  done<br> <br>注意：启动服务前必须先创建工作目录；<br> <br>检查启动结果：<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "systemctl status kube-proxy|grep Active"<br>  done<br> <br>预期结果：<br>&gt;&gt;&gt; 172.16.60.244<br>   Active: active (running) since Mon 2019-06-24 20:35:31 CST; 2min 0s ago<br>&gt;&gt;&gt; 172.16.60.245<br>   Active: active (running) since Mon 2019-06-24 20:35:30 CST; 2min 0s ago<br>&gt;&gt;&gt; 172.16.60.246<br>   Active: active (running) since Mon 2019-06-24 20:35:32 CST; 1min 59s ago<br> <br>确保状态为 active (running)，否则查看日志，确认原因（journalctl -u kube-proxy）<br> <br>7）查看监听端口（在任意一台node节点上查看）<br>[root@k8s-node01 ~]\# netstat -lnpt|grep kube-prox<br>tcp        0      0 172.16.60.244:10249     0.0.0.0:\*               LISTEN      3830/kube-proxy    <br>tcp        0      0 172.16.60.244:10256     0.0.0.0:\*               LISTEN      3830/kube-proxy  <br> <br>需要注意：<br>10249：该端口用于http prometheus metrics port;<br>10256：该端口用于http healthz port;<br> <br>8）查看 ipvs 路由规则<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "/usr/sbin/ipvsadm -ln"<br>  done<br> <br>预期输出：<br>&gt;&gt;&gt; 172.16.60.244<br>IP Virtual Server version 1.2.1 (size=4096)<br>Prot LocalAddress:Port Scheduler Flags<br>  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn<br>TCP  10.254.0.1:443 rr<br>  -&gt; 172.16.60.241:6443           Masq    1      0          0        <br>  -&gt; 172.16.60.242:6443           Masq    1      0          0        <br>  -&gt; 172.16.60.243:6443           Masq    1      0          0        <br>&gt;&gt;&gt; 172.16.60.245<br>IP Virtual Server version 1.2.1 (size=4096)<br>Prot LocalAddress:Port Scheduler Flags<br>  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn<br>TCP  10.254.0.1:443 rr<br>  -&gt; 172.16.60.241:6443           Masq    1      0          0        <br>  -&gt; 172.16.60.242:6443           Masq    1      0          0        <br>  -&gt; 172.16.60.243:6443           Masq    1      0          0        <br>&gt;&gt;&gt; 172.16.60.246<br>IP Virtual Server version 1.2.1 (size=4096)<br>Prot LocalAddress:Port Scheduler Flags<br>  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn<br>TCP  10.254.0.1:443 rr<br>  -&gt; 172.16.60.241:6443           Masq    1      0          0        <br>  -&gt; 172.16.60.242:6443           Masq    1      0          0        <br>  -&gt; 172.16.60.243:6443           Masq    1      0          0<br> <br>由上面可以看出：所有通过 https 访问 K8S SVC kubernetes 的请求都转发到 kube-apiserver 节点的 6443 端口； |


十、验证Kubernetes集群功能

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156 | 使用 daemonset 验证 master 和 worker 节点是否工作正常。<br> <br>1）检查节点状态<br>[root@k8s-master01 ~]\# kubectl get nodes<br>NAME         STATUS   ROLES    AGE    VERSION<br>k8s-node01   Ready    &lt;none&gt;   6d3h   v1.14.2<br>k8s-node02   Ready    &lt;none&gt;   6d3h   v1.14.2<br>k8s-node03   Ready    &lt;none&gt;   6d3h   v1.14.2<br> <br>各node节点状态都为 Ready 时正常。<br> <br>2）创建测试文件<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# cat &gt; nginx-ds.yml &lt;&lt;EOF<br>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: nginx-ds<br>  labels:<br>    app: nginx-ds<br>spec:<br>  type: NodePort<br>  selector:<br>    app: nginx-ds<br>  ports:<br>  - name: http<br>    port: 80<br>    targetPort: 80<br>---<br>apiVersion: extensions/v1beta1<br>kind: DaemonSet<br>metadata:<br>  name: nginx-ds<br>  labels:<br>    addonmanager.kubernetes.io/mode: Reconcile<br>spec:<br>  template:<br>    metadata:<br>      labels:<br>        app: nginx-ds<br>    spec:<br>      containers:<br>      - name: my-nginx<br>        image: nginx:1.7.9<br>        ports:<br>        - containerPort: 80<br>EOF<br> <br>执行测试<br>[root@k8s-master01 work]\# kubectl create -f nginx-ds.yml<br> <br>3）检查各节点的 Pod IP 连通性<br>稍微等一会儿，或者或刷几次下面的命令，才会显示出Pod的IP信息<br>[root@k8s-master01 work]\# kubectl get pods  -o wide|grep nginx-ds<br>nginx-ds-4lf8z   1/1     Running   0          46s   172.30.56.2   k8s-node02   &lt;none&gt;           &lt;none&gt;<br>nginx-ds-6kfsw   1/1     Running   0          46s   172.30.72.2   k8s-node03   &lt;none&gt;           &lt;none&gt;<br>nginx-ds-xqdgw   1/1     Running   0          46s   172.30.88.2   k8s-node01   &lt;none&gt;           &lt;none&gt;<br> <br>可见，nginx-ds的 Pod IP分别是 172.30.56.2、172.30.72.2、172.30.88.2，在所有 Node 上分别 ping 这三个 IP，看是否连通：<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh ${node\_node\_ip} "ping -c 1 172.30.56.2"<br>    ssh ${node\_node\_ip} "ping -c 1 172.30.72.2"<br>    ssh ${node\_node\_ip} "ping -c 1 172.30.88.2"<br>  done<br> <br>预期输出结果：<br>&gt;&gt;&gt; 172.16.60.244<br>PING 172.30.56.2 (172.30.56.2) 56(84) bytes of data.<br>64 bytes from 172.30.56.2: icmp\_seq=1 ttl=63 time=0.542 ms<br> <br>--- 172.30.56.2 ping statistics ---<br>1 packets transmitted, 1 received, 0% packet loss, time 0ms<br>rtt min/avg/max/mdev = 0.542/0.542/0.542/0.000 ms<br>PING 172.30.72.2 (172.30.72.2) 56(84) bytes of data.<br>64 bytes from 172.30.72.2: icmp\_seq=1 ttl=63 time=0.654 ms<br> <br>--- 172.30.72.2 ping statistics ---<br>1 packets transmitted, 1 received, 0% packet loss, time 0ms<br>rtt min/avg/max/mdev = 0.654/0.654/0.654/0.000 ms<br>PING 172.30.88.2 (172.30.88.2) 56(84) bytes of data.<br>64 bytes from 172.30.88.2: icmp\_seq=1 ttl=64 time=0.103 ms<br> <br>--- 172.30.88.2 ping statistics ---<br>1 packets transmitted, 1 received, 0% packet loss, time 0ms<br>rtt min/avg/max/mdev = 0.103/0.103/0.103/0.000 ms<br>&gt;&gt;&gt; 172.16.60.245<br>PING 172.30.56.2 (172.30.56.2) 56(84) bytes of data.<br>64 bytes from 172.30.56.2: icmp\_seq=1 ttl=64 time=0.106 ms<br> <br>--- 172.30.56.2 ping statistics ---<br>1 packets transmitted, 1 received, 0% packet loss, time 0ms<br>rtt min/avg/max/mdev = 0.106/0.106/0.106/0.000 ms<br>PING 172.30.72.2 (172.30.72.2) 56(84) bytes of data.<br>64 bytes from 172.30.72.2: icmp\_seq=1 ttl=63 time=0.408 ms<br> <br>--- 172.30.72.2 ping statistics ---<br>1 packets transmitted, 1 received, 0% packet loss, time 0ms<br>rtt min/avg/max/mdev = 0.408/0.408/0.408/0.000 ms<br>PING 172.30.88.2 (172.30.88.2) 56(84) bytes of data.<br>64 bytes from 172.30.88.2: icmp\_seq=1 ttl=63 time=0.345 ms<br> <br>--- 172.30.88.2 ping statistics ---<br>1 packets transmitted, 1 received, 0% packet loss, time 0ms<br>rtt min/avg/max/mdev = 0.345/0.345/0.345/0.000 ms<br>&gt;&gt;&gt; 172.16.60.246<br>PING 172.30.56.2 (172.30.56.2) 56(84) bytes of data.<br>64 bytes from 172.30.56.2: icmp\_seq=1 ttl=63 time=0.350 ms<br> <br>--- 172.30.56.2 ping statistics ---<br>1 packets transmitted, 1 received, 0% packet loss, time 0ms<br>rtt min/avg/max/mdev = 0.350/0.350/0.350/0.000 ms<br>PING 172.30.72.2 (172.30.72.2) 56(84) bytes of data.<br>64 bytes from 172.30.72.2: icmp\_seq=1 ttl=64 time=0.105 ms<br> <br>--- 172.30.72.2 ping statistics ---<br>1 packets transmitted, 1 received, 0% packet loss, time 0ms<br>rtt min/avg/max/mdev = 0.105/0.105/0.105/0.000 ms<br>PING 172.30.88.2 (172.30.88.2) 56(84) bytes of data.<br>64 bytes from 172.30.88.2: icmp\_seq=1 ttl=63 time=0.584 ms<br> <br>--- 172.30.88.2 ping statistics ---<br>1 packets transmitted, 1 received, 0% packet loss, time 0ms<br>rtt min/avg/max/mdev = 0.584/0.584/0.584/0.000 ms<br> <br>4）检查服务 IP 和端口可达性<br>[root@k8s-master01 work]\# kubectl get svc |grep nginx-ds<br>nginx-ds     NodePort    10.254.41.83   &lt;none&gt;        80:30876/TCP   4m24s<br> <br>可见：<br>Service Cluster IP：10.254.41.83<br>服务端口：80<br>NodePort 端口：30876<br> <br>在所有 Node 上 curl Service IP：<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh ${node\_node\_ip} "curl -s 10.254.41.83"<br>  done<br> <br>预期输出: nginx欢迎页面内容。<br> <br>5）检查服务的 NodePort 可达性<br>在所有 Node 上执行：<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh ${node\_node\_ip} "curl -s ${node\_node\_ip}:30876"<br>  done<br> <br>预期输出: nginx 欢迎页面内容。 |


*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************

分类: Docker

好文要顶 关注我 收藏该文 

![](images/865FE72A18DA43A3B02757AB49FDEB0Ccon_weibo_24.png)

 

![](images/C72CAF4C8C574E1392E8DE172D4920F2wechat.png)

![](images/0AD1560BAC8C492B9926B9C872CDCE3D161124180837.png)

散尽浮华

关注 - 23

粉丝 - 3133

+加关注

0

0

« 上一篇： 应用指标数据采集并录入Elasticsearch仓库 - 运维笔记

» 下一篇： Kubernetes容器集群管理环境 - 完整部署（下篇）

posted @ 2019-07-03 20:50  散尽浮华  阅读(8676)  评论(3)  编辑  收藏



评论

  

#1楼 2019-08-11 11:32 | zzf.zzf

systemctl status kube-scheduler启动服务时，报错：unknown flag: --bind-address 后面的参数有问题..



--bind-address=0.0.0.0 \\

--tls-cert-file=/etc/kubernetes/cert/kube-scheduler.pem \\

--tls-private-key-file=/etc/kubernetes/cert/kube-scheduler-key.pem \\

--authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\

--client-ca-file=/etc/kubernetes/cert/ca.pem \\

--requestheader-allowed-names="" \\

--requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\

--requestheader-extra-headers-prefix="X-Remote-Extra-" \\

--requestheader-group-headers=X-Remote-Group \\

--requestheader-username-headers=X-Remote-User \\

大师帮忙看看？

支持(0) 反对(0)

  

#2楼 2021-02-01 14:55 | XWD2020

赞，试验成功。

支持(0) 反对(0)

  

#3楼 2021-02-02 11:14 | XWD2020

创建几个nginx服务时候 容器很久没有起来，提示 ContainerCreating

原来先要下载镜像 pause-amd64:3.1

报错：

[root@k8s-master01 work]# kubectl get pods -o wide

nginx-ds-58hfx 0/1 ContainerCreating 0 55m <none> k8s-node01 <none> <none>

查看报错原因：kubectl describe pod nginx-ds-58hfx

Failed create pod sandbox: rpc error: code = Unknown desc = failed pulling image "registry.cn-beijing.aliyuncs.com/k8s_images/pause-amd64:3.1": Error response from daemon: pull access denied for registry.cn-beijing.aliyuncs.com/k8s_images/pause-amd64, repository does not exist or may require 'docker login'

原因是没有这个地址：

root@k8s-node01 ~]# docker pull registry.cn-beijing.aliyuncs.com/k8s_images/pause-amd64:3.1

Error response from daemon: pull access denied for registry.cn-beijing.aliyuncs.com/k8s_images/pause-amd64, repository does not exist or may require 'docker login'

这样就可以，在全部node上先下载docker images

docker pull registry.cn-beijing.aliyuncs.com/zhoujun/pause-amd64:3.1

或者

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.0

然后修改版本：

docker tag registry.cn-beijing.aliyuncs.com/zhoujun/pause-amd64:3.1 registry.cn-beijing.aliyuncs.com/k8s_images/pause-amd64:3.1

在node上查看镜像 有了，名字也改好了

查看master

[root@k8s-master01 work]# kubectl get pods -o wide

NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES

nginx-ds-58hfx 1/1 Running 成功