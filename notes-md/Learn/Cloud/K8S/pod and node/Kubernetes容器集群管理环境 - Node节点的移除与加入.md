 

一、如何从Kubernetes集群中移除Node

比如从集群中移除k8s-node03这个Node节点，做法如下：

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79 | 1）先在master节点查看Node情况<br>[root@k8s-master01 ~]\# kubectl get nodes<br>NAME         STATUS   ROLES    AGE   VERSION<br>k8s-node01   Ready    &lt;none&gt;   47d   v1.14.2<br>k8s-node02   Ready    &lt;none&gt;   47d   v1.14.2<br>k8s-node03   Ready    &lt;none&gt;   47d   v1.14.2<br> <br>2）接着查看下pod情况<br>[root@k8s-master01 ~]\# kubectl get pods -o wide<br>NAME                        READY   STATUS    RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES<br>dnsutils-ds-5sc4z           1/1     Running   963        40d   172.30.56.3    k8s-node02   &lt;none&gt;           &lt;none&gt;<br>dnsutils-ds-h546r           1/1     Running   963        40d   172.30.72.5    k8s-node03   &lt;none&gt;           &lt;none&gt;<br>dnsutils-ds-jx5kx           1/1     Running   963        40d   172.30.88.4    k8s-node01   &lt;none&gt;           &lt;none&gt;<br>kevin-nginx                 1/1     Running   0          27d   172.30.72.11   k8s-node03   &lt;none&gt;           &lt;none&gt;<br>my-nginx-5dd67b97fb-69gvm   1/1     Running   0          40d   172.30.72.4    k8s-node03   &lt;none&gt;           &lt;none&gt;<br>my-nginx-5dd67b97fb-8j4k6   1/1     Running   0          40d   172.30.88.3    k8s-node01   &lt;none&gt;           &lt;none&gt;<br>nginx-7db9fccd9b-dkdzf      1/1     Running   0          27d   172.30.88.8    k8s-node01   &lt;none&gt;           &lt;none&gt;<br>nginx-7db9fccd9b-t8njb      1/1     Running   0          27d   172.30.72.10   k8s-node03   &lt;none&gt;           &lt;none&gt;<br>nginx-7db9fccd9b-vrp9f      1/1     Running   0          27d   172.30.56.6    k8s-node02   &lt;none&gt;           &lt;none&gt;<br>nginx-ds-4lf8z              1/1     Running   0          41d   172.30.56.2    k8s-node02   &lt;none&gt;           &lt;none&gt;<br>nginx-ds-6kfsw              1/1     Running   0          41d   172.30.72.2    k8s-node03   &lt;none&gt;           &lt;none&gt;<br>nginx-ds-xqdgw              1/1     Running   0          41d   172.30.88.2    k8s-node01   &lt;none&gt;           &lt;none&gt;<br> <br>3）封锁k8s-node03这个node节点，排干该node节点上的pod资源<br>[root@k8s-master01 ~]\# kubectl drain k8s-node03 --delete-local-data --force --ignore-daemonsets<br>node/k8s-node03 cordoned<br>WARNING: deleting Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet: default/kevin-nginx; ignoring DaemonSet-managed Pods: default/dnsutils-ds-h546r, default/nginx-ds-6kfsw, kube-system/node-exporter-zmb68<br>evicting pod "metrics-server-54997795d9-rczmc"<br>evicting pod "kevin-nginx"<br>evicting pod "nginx-7db9fccd9b-t8njb"<br>evicting pod "coredns-5b969f4c88-pd5js"<br>evicting pod "kubernetes-dashboard-7976c5cb9c-4jpzb"<br>evicting pod "my-nginx-5dd67b97fb-69gvm"<br>pod/my-nginx-5dd67b97fb-69gvm evicted<br>pod/coredns-5b969f4c88-pd5js evicted<br>pod/nginx-7db9fccd9b-t8njb evicted<br>pod/kubernetes-dashboard-7976c5cb9c-4jpzb evicted<br>pod/kevin-nginx evicted<br>pod/metrics-server-54997795d9-rczmc evicted<br>node/k8s-node03 evicted<br> <br>4）接着删除k8s-node03这个节点<br>[root@k8s-master01 ~]\# kubectl delete node k8s-node03<br>node "k8s-node03" deleted<br> <br>5）再查看pod情况，发现原来在k8s-node03上的pod已经调度到其他留存的node节点上了<br>[root@k8s-master01 ~]\# kubectl get pods -o wide<br>NAME                        READY   STATUS    RESTARTS   AGE   IP            NODE         NOMINATED NODE   READINESS GATES<br>dnsutils-ds-5sc4z           1/1     Running   963        40d   172.30.56.3   k8s-node02   &lt;none&gt;           &lt;none&gt;<br>dnsutils-ds-jx5kx           1/1     Running   963        40d   172.30.88.4   k8s-node01   &lt;none&gt;           &lt;none&gt;<br>my-nginx-5dd67b97fb-8j4k6   1/1     Running   0          40d   172.30.88.3   k8s-node01   &lt;none&gt;           &lt;none&gt;<br>my-nginx-5dd67b97fb-kx2pc   1/1     Running   0          98s   172.30.56.7   k8s-node02   &lt;none&gt;           &lt;none&gt;<br>nginx-7db9fccd9b-7vbhq      1/1     Running   0          98s   172.30.88.7   k8s-node01   &lt;none&gt;           &lt;none&gt;<br>nginx-7db9fccd9b-dkdzf      1/1     Running   0          27d   172.30.88.8   k8s-node01   &lt;none&gt;           &lt;none&gt;<br>nginx-7db9fccd9b-vrp9f      1/1     Running   0          27d   172.30.56.6   k8s-node02   &lt;none&gt;           &lt;none&gt;<br>nginx-ds-4lf8z              1/1     Running   0          41d   172.30.56.2   k8s-node02   &lt;none&gt;           &lt;none&gt;<br>nginx-ds-xqdgw              1/1     Running   0          41d   172.30.88.2   k8s-node01   &lt;none&gt;           &lt;none&gt;<br> <br>[root@k8s-master01 ~]\# kubectl get nodes<br>NAME         STATUS   ROLES    AGE   VERSION<br>k8s-node01   Ready    &lt;none&gt;   47d   v1.14.2<br>k8s-node02   Ready    &lt;none&gt;   47d   v1.14.2<br> <br>6）最后在k8s-node03节点上执行清理操作：<br>[root@k8s-node03 ~]\# systemctl stop kubelet kube-proxy flanneld docker<br>  <br>[root@k8s-node03 ~]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-node03 ~]\# mount | grep "${K8S\_DIR}" | awk '{print $3}'|xargs sudo umount<br>[root@k8s-node03 ~]\# rm -rf ${K8S\_DIR}/kubelet<br>[root@k8s-node03 ~]\# rm -rf ${DOCKER\_DIR}<br>[root@k8s-node03 ~]\# rm -rf /var/run/flannel/<br>[root@k8s-node03 ~]\# rm -rf /var/run/docker/<br>[root@k8s-node03 ~]\# rm -rf /etc/systemd/system/{kubelet,docker,flanneld,kube-nginx}.service<br>[root@k8s-node03 ~]\# rm -rf /opt/k8s/bin/\*<br>[root@k8s-node03 ~]\# rm -rf /etc/flanneld/cert /etc/kubernetes/cert<br>  <br>[root@k8s-node03 ~]\# iptables -F &amp;&amp; iptables -X &amp;&amp; iptables -F -t nat &amp;&amp; iptables -X -t nat<br>[root@k8s-node03 ~]\# ip link del flannel.1<br>[root@k8s-node03 ~]\# ip link del docker0 |


二、如何向Kubernetes集群中加入Node节点

比如将之前移除的k8s-node03节点重新加入到k8s集群中 （下面操作都在k8s-master01节点上完成）

|   |   |
| - | - |
| 1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335 | 1）修改变量脚本文件/opt/k8s/bin/environment.sh里的NODE节点为k8s-node03节点，然后进行分发。<br>[root@k8s-master01 ~]\# cp /opt/k8s/bin/environment.sh /opt/k8s/bin/environment.sh.bak1<br>[root@k8s-master01 ~]\# vim /opt/k8s/bin/environment.sh<br>........<br>\# 集群中所有node节点集群IP数组<br>export NODE\_NODE\_IPS=(172.16.60.246)<br>\# 集群中node节点IP对应的主机名数组<br>export NODE\_NODE\_NAMES=(k8s-node03)<br>   <br>[root@k8s-master01 ~]\# diff /opt/k8s/bin/environment.sh /opt/k8s/bin/environment.sh.bak1<br>17c17<br>&lt; export NODE\_NODE\_IPS=(172.16.60.246)<br>---<br>&gt; export NODE\_NODE\_IPS=(172.16.60.244 172.16.60.245 172.16.60.246)<br>19c19<br>&lt; export NODE\_NODE\_NAMES=(k8s-node03)<br>---<br>&gt; export NODE\_NODE\_NAMES=(k8s-node01 k8s-node02 k8s-node03)<br>   <br>2）将之前在k8s-master01节点上生产的证书文件分发到新加入的node节点上<br>[root@k8s-master01 ~]\# cd /opt/k8s/work/<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "mkdir -p /etc/kubernetes/cert"<br>    scp ca\*.pem ca-config.json root@${node\_node\_ip}:/etc/kubernetes/cert<br>  done<br>   <br>3) Flannel容器网络<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    scp flannel/{flanneld,mk-docker-opts.sh} root@${node\_node\_ip}:/opt/k8s/bin/<br>    ssh root@${node\_node\_ip} "chmod +x /opt/k8s/bin/\*"<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "mkdir -p /etc/flanneld/cert"<br>    scp flanneld\*.pem root@${node\_node\_ip}:/etc/flanneld/cert<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    scp flanneld.service root@${node\_node\_ip}:/etc/systemd/system/<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld"<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "systemctl status flanneld|grep Active"<br>  done<br>   <br>4）部署node节点运行组件<br>   <br>-&gt;  安装依赖包<br>[root@k8s-master01 ~]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 ~]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "yum install -y epel-release"<br>    ssh root@${node\_node\_ip} "yum install -y conntrack ipvsadm ntp ntpdate ipset jq iptables curl sysstat libseccomp &amp;&amp; modprobe ip\_vs "<br>  done<br>   <br>-&gt;  部署docker组件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    scp docker/\*  root@${node\_node\_ip}:/opt/k8s/bin/<br>    ssh root@${node\_node\_ip} "chmod +x /opt/k8s/bin/\*"<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    scp docker.service root@${node\_node\_ip}:/etc/systemd/system/<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "mkdir -p  /etc/docker/ ${DOCKER\_DIR}/{data,exec}"<br>    scp docker-daemon.json root@${node\_node\_ip}:/etc/docker/daemon.json<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker"<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "systemctl status docker|grep Active"<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "/usr/sbin/ip addr show flannel.1 &amp;&amp; /usr/sbin/ip addr show docker0"<br>  done<br>   <br>-&gt;  部署kubelet组件<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    scp kubernetes/server/bin/kubelet root@${node\_node\_ip}:/opt/k8s/bin/<br>    ssh root@${node\_node\_ip} "chmod +x /opt/k8s/bin/\*"<br>  done<br>   <br>-&gt;  创建token（之前创建的已经过期，token有效期只有24h，即有效期只有一天！）<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_name in ${NODE\_NODE\_NAMES[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_name}"<br>       <br>    \# 创建 token<br>    export BOOTSTRAP\_TOKEN=$(kubeadm token create \\<br>      --description kubelet-bootstrap-token \\<br>      --groups system:bootstrappers:${node\_node\_name} \\<br>      --kubeconfig ~/.kube/config)<br>       <br>    \# 设置集群参数<br>    kubectl config set-cluster kubernetes \\<br>      --certificate-authority=/etc/kubernetes/cert/ca.pem \\<br>      --embed-certs=true \\<br>      --server=${KUBE\_APISERVER} \\<br>      --kubeconfig=kubelet-bootstrap-${node\_node\_name}.kubeconfig<br>       <br>    \# 设置客户端认证参数<br>    kubectl config set-credentials kubelet-bootstrap \\<br>      --token=${BOOTSTRAP\_TOKEN} \\<br>      --kubeconfig=kubelet-bootstrap-${node\_node\_name}.kubeconfig<br>       <br>    \# 设置上下文参数<br>    kubectl config set-context default \\<br>      --cluster=kubernetes \\<br>      --user=kubelet-bootstrap \\<br>      --kubeconfig=kubelet-bootstrap-${node\_node\_name}.kubeconfig<br>       <br>    \# 设置默认上下文<br>    kubectl config use-context default --kubeconfig=kubelet-bootstrap-${node\_node\_name}.kubeconfig<br>  done<br>   <br>查看 kubeadm 为各新节点创建的 token：<br>[root@k8s-master01 work]\# kubeadm token list --kubeconfig ~/.kube/config<br>TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION               EXTRA GROUPS<br>sdwq5g.llzr9ytm32h1mnh1   23h       2019-08-06T11:47:47+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:k8s-node03<br>   <br>[root@k8s-master01 work]\# kubectl get secrets  -n kube-system|grep bootstrap-token<br>bootstrap-token-sdwq5g                           bootstrap.kubernetes.io/token         7      77s<br>   <br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_name in ${NODE\_NODE\_NAMES[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_name}"<br>    scp kubelet-bootstrap-${node\_node\_name}.kubeconfig root@${node\_node\_name}:/etc/kubernetes/kubelet-bootstrap.kubeconfig<br>  done<br>   <br>-&gt;  分发 bootstrap kubeconfig 文件到新增node节点<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_name in ${NODE\_NODE\_NAMES[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_name}"<br>    scp kubelet-bootstrap-${node\_node\_name}.kubeconfig root@${node\_node\_name}:/etc/kubernetes/kubelet-bootstrap.kubeconfig<br>  done<br>   <br>-&gt;  分发 kubelet 参数配置文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    sed -e "s/\#\#NODE\_NODE\_IP\#\#/${node\_node\_ip}/" kubelet-config.yaml.template &gt; kubelet-config-${node\_node\_ip}.yaml.template<br>    scp kubelet-config-${node\_node\_ip}.yaml.template root@${node\_node\_ip}:/etc/kubernetes/kubelet-config.yaml<br>  done<br>   <br>-&gt;  分发 kubelet systemd unit 文件<br>[root@k8s-master01 work]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_name in ${NODE\_NODE\_NAMES[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_name}"<br>    sed -e "s/\#\#NODE\_NODE\_NAME\#\#/${node\_node\_name}/" kubelet.service.template &gt; kubelet-${node\_node\_name}.service<br>    scp kubelet-${node\_node\_name}.service root@${node\_node\_name}:/etc/systemd/system/kubelet.service<br>  done<br>   <br>-&gt;  启动 kubelet 服务<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "mkdir -p ${K8S\_DIR}/kubelet/kubelet-plugins/volume/exec/"<br>    ssh root@${node\_node\_ip} "/usr/sbin/swapoff -a"<br>    ssh root@${node\_node\_ip} "systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet"<br>  done<br>  <br>-&gt; 部署 kube-proxy 组件<br>[root@k8s-master01 ~]\# cd /opt/k8s/work<br>[root@k8s-master01 work]\# source /opt/k8s/bin/environment.sh<br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    scp kubernetes/server/bin/kube-proxy root@${node\_node\_ip}:/opt/k8s/bin/<br>    ssh root@${node\_node\_ip} "chmod +x /opt/k8s/bin/\*"<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_name in ${NODE\_NODE\_NAMES[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_name}"<br>    scp kube-proxy.kubeconfig root@${node\_node\_name}:/etc/kubernetes/<br>  done<br>  <br>=================================================================================================================================================<br>特别注意（如果是完全新增node节点，则这里需要添加下面操作）：<br>由于这里是恢复之前移除的k8s-node03节点，故这里不需要重新根据kube-proxy配置模板生成对应的新增node节点的配置文件(因为之前已经生成过了)<br>[root@k8s-master01 work]\# ll kube-proxy-config-k8s-node\*<br>-rw-r--r-- 1 root root 500 Jun 24 20:27 kube-proxy-config-k8s-node01.yaml.template<br>-rw-r--r-- 1 root root 500 Jun 24 20:27 kube-proxy-config-k8s-node02.yaml.template<br>-rw-r--r-- 1 root root 500 Jun 24 20:27 kube-proxy-config-k8s-node03.yaml.template<br>  <br>如果是完全新增加的节点，比如新增加的node节点172.16.60.240 (主机名: k8s-node04)，<br>则这一步还需要拷贝已存在node节点的配置文件为新增node节点的配置文件，然后分发过去<br>[root@k8s-master01 work]\# cp kube-proxy-config-k8s-node03.yaml.template kube-proxy-config-k8s-node04.yaml.template<br>[root@k8s-master01 work]\# sed -i 's/172.16.60.246/172.16.60.240/g' kube-proxy-config-k8s-node04.yaml.template<br>[root@k8s-master01 work]\# sed -i 's/k8s-node03/k8s-node04/g' kube-proxy-config-k8s-node04.yaml.template<br>[root@k8s-master01 work]\# scp kube-proxy-config-k8s-node04.yaml.template root@k8s-node04:/etc/kubernetes/kube-proxy-config.yaml<br>  <br>如果是新增多个node节点，则同样是拷贝已存在node节点的配置文件为各个新增node节点的配置文件，然后分发过去<br>=================================================================================================================================================<br>  <br>[root@k8s-master01 work]\# for node\_node\_name in ${NODE\_NODE\_NAMES[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_name}"<br>    scp kube-proxy.service root@${node\_node\_name}:/etc/systemd/system/<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "mkdir -p ${K8S\_DIR}/kube-proxy"<br>    ssh root@${node\_node\_ip} "modprobe ip\_vs\_rr"<br>    ssh root@${node\_node\_ip} "systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy"<br>  done<br>   <br>[root@k8s-master01 work]\# for node\_node\_ip in ${NODE\_NODE\_IPS[@]}<br>  do<br>    echo "&gt;&gt;&gt; ${node\_node\_ip}"<br>    ssh root@${node\_node\_ip} "systemctl status kube-proxy|grep Active"<br>  done<br>   <br>-&gt;  手动 approve server cert csr<br>[root@k8s-master01 work]\# kubectl get csr<br>NAME        AGE     REQUESTOR                 CONDITION<br>csr-5fwlh   3m34s   system:bootstrap:sdwq5g   Approved,Issued<br>csr-t547p   3m21s   system:node:k8s-node03    Pending<br>   <br>[root@k8s-master01 work]\# kubectl certificate approve csr-t547p<br>certificatesigningrequest.certificates.k8s.io/csr-t547p approved<br>   <br>[root@k8s-master01 work]\# kubectl get csr<br>NAME        AGE     REQUESTOR                 CONDITION<br>csr-5fwlh   3m53s   system:bootstrap:sdwq5g   Approved,Issued<br>csr-t547p   3m40s   system:node:k8s-node03    Approved,Issued<br>   <br>-&gt; 查看集群状态，发现k8s-node03节点已经被重新加入到集群中了，并且已经分配了pod资源。<br>[root@k8s-master01 work]\# kubectl get nodes<br>NAME         STATUS   ROLES    AGE   VERSION<br>k8s-node01   Ready    &lt;none&gt;   47d   v1.14.2<br>k8s-node02   Ready    &lt;none&gt;   47d   v1.14.2<br>k8s-node03   Ready    &lt;none&gt;   1s    v1.14.2<br>   <br>[root@k8s-master01 work]\# kubectl get pods -o wide<br>NAME                        READY   STATUS    RESTARTS   AGE    IP            NODE         NOMINATED NODE   READINESS GATES<br>dnsutils-ds-5sc4z           1/1     Running   965        40d    172.30.56.3   k8s-node02   &lt;none&gt;           &lt;none&gt;<br>dnsutils-ds-gc8sb           1/1     Running   1          94m    172.30.72.2   k8s-node03   &lt;none&gt;           &lt;none&gt;<br>dnsutils-ds-jx5kx           1/1     Running   966        40d    172.30.88.4   k8s-node01   &lt;none&gt;           &lt;none&gt;<br>my-nginx-5dd67b97fb-8j4k6   1/1     Running   0          40d    172.30.88.3   k8s-node01   &lt;none&gt;           &lt;none&gt;<br>my-nginx-5dd67b97fb-kx2pc   1/1     Running   0          174m   172.30.56.7   k8s-node02   &lt;none&gt;           &lt;none&gt;<br>nginx-7db9fccd9b-7vbhq      1/1     Running   0          174m   172.30.88.7   k8s-node01   &lt;none&gt;           &lt;none&gt;<br>nginx-7db9fccd9b-dkdzf      1/1     Running   0          27d    172.30.88.8   k8s-node01   &lt;none&gt;           &lt;none&gt;<br>nginx-7db9fccd9b-vrp9f      1/1     Running   0          27d    172.30.56.6   k8s-node02   &lt;none&gt;           &lt;none&gt;<br>nginx-ds-4lf8z              1/1     Running   0          41d    172.30.56.2   k8s-node02   &lt;none&gt;           &lt;none&gt;<br>nginx-ds-jn759              1/1     Running   0          94m    172.30.72.3   k8s-node03   &lt;none&gt;           &lt;none&gt;<br>nginx-ds-xqdgw              1/1     Running   0          41d    172.30.88.2   k8s-node01   &lt;none&gt;           &lt;none&gt;<br>   <br>[root@k8s-master01 work]\# kubectl top node<br>NAME         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%<br>k8s-node01   96m          2%     2123Mi          55%    <br>k8s-node02   133m         3%     1772Mi          46%    <br>k8s-node03   46m          1%     4859Mi          61%<br>   <br>=======================================================================================================================================<br>注意一<br>如果是添加全新的节点到上述k8s集群中，做法如下：<br>1）做好node节点的环境初始化准备，如做好K8s-master01到新增节点的ssh无密码登录的信任关系；etc/hosts里做好绑定；关闭防火墙等。<br>2）在/opt/k8s/bin/environment.sh变量脚本里，将NODE\_NODE\_IPS和NODE\_NODE\_NAMES变量改成新增node节点的对应信息<br>3）按照上面添加k8s-node03节点的一系列添加步骤全部执行一遍即可<br>   <br>======================================================================================================================================<br>注意二<br>上面使用的是二进制方式按照k8s集群。如果使用kubeadmin工具创建的k8s集群，则重新使node加入集群的操作如下：<br>   <br>使节点加入集群的命令格式（node节点上操作，使用root用户）：<br>\# kubeadm join --token &lt;token&gt; &lt;master-ip&gt;:&lt;master-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;<br>   <br>如果忘记了Master节点的token，可以使用下面命令查看（master节点上操作）：<br>\# kubeadm token list<br>   <br>默认情况下，token的有效期是24小时，如果token已经过期的话，可以使用下面命令重新生成（master节点上操作）;<br>\# kubeadm token create<br>   <br>如果找不到--discovery-token-ca-cert-hash的值，可以使用以下命令生成（master节点上操作）：<br>\# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.\* //'<br>   <br>加入节点后，稍等一会儿，即可看到节点已加入（master节点上操作） |


*************** 当你发现自己的才华撑不起野心时，就请安静下来学习吧！***************